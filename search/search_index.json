{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"contributing/build-from-source/","text":"Building from source \u00b6 This document describes how to build HStreamDB from source code. Building with cabal \u00b6 TODO Building with stack \u00b6 TODO","title":"Building from source"},{"location":"contributing/build-from-source/#building-from-source","text":"This document describes how to build HStreamDB from source code.","title":"Building from source"},{"location":"contributing/build-from-source/#building-with-cabal","text":"TODO","title":"Building with cabal"},{"location":"contributing/build-from-source/#building-with-stack","text":"TODO","title":"Building with stack"},{"location":"contributing/haskell-style/","text":"Haskell Style Guide \u00b6 This document is a slightly modified version of style guide used in Kowainik . Style guide goals \u00b6 The purpose of this document is to help developers and people working on Haskell code-bases to have a smoother experience while dealing with code in different situations. This style guide aims to increase productivity by defining the following goals: Make code easier to understand: ideas for solutions should not be hidden behind complex and obscure code. Make code easier to read: code arrangement should be immediately apparent after looking at the existing code. Names of functions & variables should be transparent and obvious. Make code easier to write: developers should think about code formatting rules as little as possible. The style guide should answer any query pertaining to the formatting of a specific piece of code. Make code easier to maintain: this style guide aims to reduce the burden of maintaining packages using version control systems unless this conflicts with the previous points. Rule of thumb when working with existing source code The general rule is to stick to the same coding style that is already used in the file you are editing. If you must make significant style modifications, then commit them independently from the functional changes so that someone looking back through the changelog can easily distinguish between them. Indentation \u00b6 Indent code blocks with 2 spaces . Always put a where keyword on a new line. showSign :: Int -> String showSign n | n == 0 = \"Zero\" | n < 0 = \"Negative\" | otherwise = \"Positive\" greet :: IO () greet = do putStrLn \"What is your name?\" name <- getLine putStrLn $ greeting name where greeting :: String -> String greeting name = \"Hey \" ++ name ++ \"!\" Line length \u00b6 The maximum preferred line length is 80 characters . Tip There is no hard rules when it comes to line length. Some lines just have to be a bit longer than usual. However, if your line of code exceeds this limit, try to split code into smaller chunks or break long lines over multiple shorter ones as much as you can. Whitespaces \u00b6 No trailing whitespaces (use some tools to automatically cleanup trailing whitespaces). Surround binary operators with a single space on either side. Alignment \u00b6 Use comma-leading style for formatting module exports, lists, tuples, records, etc. answers :: [ Maybe Int ] answers = [ Just 42 , Just 7 , Nothing ] If a function definition doesn't fit the line limit then align multiple lines according to the same separator like :: , => , -> . -- + Good printQuestion :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () -- + Acceptable if function name is short fun :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () Align records with every field on a separate line with leading commas. -- + Good data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) -- + Acceptable data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) Align sum types with every constructor on its own line with leading = and | . -- + Good data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) -- + Acceptable data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) Try to follow the above rule inside function definitions but without fanatism: -- + Good createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo -- there's no need to put the constructor on a separate line and have an extra line <$> veryLongBar <*> veryLongBaz Basically, it is often possible to join consequent lines without introducing alignment dependency. Try not to span multiple short lines unnecessarily. If a function application must spawn multiple lines to fit within the maximum line length, then write one argument on each line following the head, indented by one level: veryLongProductionName firstArgumentOfThisFunction secondArgumentOfThisFunction ( DummyDatatype withDummyField1 andDummyField2 ) lastArgumentOfThisFunction Naming \u00b6 Functions and variables \u00b6 lowerCamelCase for function and variable names. UpperCamelCase for data types, typeclasses and constructors. Variant Use ids_with_underscores for local variables only. Try not to create new operators. -- What does this 'mouse operator' mean? :thinking_suicide: ( ~@@^> ) :: Functor f => ( a -> b ) -> ( a -> c -> d ) -> ( b -> f c ) -> a -> f d Do not use ultra-short or indescriptive names like a , par , g unless the types of these variables are general enough. -- + Good mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect test ifTrue ifFalse = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if test x then ifTrue x : go xs else ifFalse x : go xs -- - Bad mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect p f g = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if p x then f x : go xs else g x : go xs Do not introduce unnecessarily long names for variables. -- + Good map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map f ( x : xs ) = f x : map f xs -- - Bad map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map function ( firstElement : remainingList ) = function firstElement : map function remainingList For readability reasons, do not capitalize all letters when using an abbreviation as a part of a longer name. For example, write TomlException instead of TOMLException . Unicode symbols are allowed only in modules that already use unicode symbols. If you create a unicode name, you should also create a non-unicode one as an alias. Data types \u00b6 Creating data types is extremely easy in Haskell. It is usually a good idea to introduce a custom data type (enum or newtype ) instead of using a commonly used data type (like Int , String , Set Text , etc.). type aliases are allowed only for specializing general types: -- + Good data StateT s m a type State s = StateT s Identity -- - Bad type Size = Int Use the data type name as the constructor name for data with single constructor and newtype . data User = User Int String The field name for a newtype must be prefixed by un followed by the type name. newtype Size = Size { unSize :: Int } newtype App a = App { unApp :: ReaderT Context IO a } Field names for the record data type should start with the full name of the data type. -- + Good data HealthReading = HealthReading { healthReadingDate :: UTCTime , healthReadingMeasurement :: Double } It is acceptable to use an abbreviation as the field prefix if the data type name is too long. -- + Acceptable data HealthReading = HealthReading { hrDate :: UTCTime , hrMeasurement :: Double } Comments \u00b6 Separate end-of-line comments from the code with 2 spaces . newtype Measure = Measure { unMeasure :: Double -- ^ See how 2 spaces separate this comment } Write Haddock documentation for the top-level functions, function arguments and data type fields. The documentation should give enough information to apply the function without looking at its definition. -- | Single-line short comment. foo :: Int -> [ a ] -> [ a ] -- | Example of multi-line block comment which is very long -- and doesn't fit single line. foo :: Int -> [ a ] -> [ a ] -- + Good -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list -> a -- ^ Element to populate list -> [ a ] -- - Bad -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list {- | Element to populate list -} -> a -> [ a ] If possible, include typeclass laws and function usage examples into the documentation. -- | The class of semigroups (types with an associative binary operation). -- -- Instances should satisfy the associativity law: -- -- * @x '<>' (y '<>' z) = (x '<>' y) '<>' z@ class Semigroup a where ( <> ) :: a -> a -> a -- | The 'intersperse' function takes a character and places it -- between the characters of a 'Text'. -- -- >>> T.intersperse '.' \"SHIELD\" -- \"S.H.I.E.L.D\" intersperse :: Char -> Text -> Text Guideline for module formatting \u00b6 Allowed tools for automatic module formatting: stylish-haskell : for formatting the import section and for alignment. LANGUAGE \u00b6 Put OPTIONS_GHC pragma before LANGUAGE pragmas in a separate section. Write each LANGUAGE pragma on its own line, sort them alphabetically and align by max width among them. {-# OPTIONS_GHC -fno-warn-orphans #-} {-# LANGUAGE ApplicativeDo #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeApplications #-} Always put language extensions in the relevant source file. Tip Language extensions must be listed at the very top of the file, above the module name. Export lists \u00b6 Use the following rules to format the export section: Always write an explicit export list. Indent the export list by 2 spaces . You can split the export list into sections. Use Haddock to assign names to these sections. Classes, data types and type aliases should be written before functions in each section. module Map ( -- * Data type Map , Key , empty -- * Update , insert , insertWith , alter ) where Imports \u00b6 Always use explicit import lists or qualified imports. Use qualified imports only if the import list is big enough or there are conflicts in names. This makes the code more robust against changes in dependent libraries. Exception: modules that only reexport other entire modules. Imports should be grouped in the following order: Imports from Hackage packages. Imports from the current project. Put a blank line between each group of imports. The imports in each group should be sorted alphabetically by module name. module MyProject.Foo ( Foo ( .. ) ) where import Control.Exception ( catch , try ) import qualified Data.Aeson as Json import qualified Data.Text as Text import Data.Traversable ( for ) import MyProject.Ansi ( errorMessage , infoMessage ) import qualified MyProject.BigModule as Big data Foo ... Data declaration \u00b6 Refer to the Alignment section to see how to format data type declarations. Records for data types with multiple constructors are forbidden. -- - Bad data Foo = Bar { bar1 :: Int , bar2 :: Double } | Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Good data Foo = FooBar Bar | FooBaz Baz data Bar = Bar { bar1 :: Int , bar2 :: Double } data Baz = Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Also good data Foo = Bar Int Double | Baz Int Double Text Strictness \u00b6 Fields of data type constructors should be strict. Specify strictness explicitly with ! . This helps to avoid space leaks and gives you an error instead of a warning in case you forget to initialize some fields. -- + Good data Settings = Settings { settingsHasTravis :: ! Bool , settingsConfigPath :: ! FilePath , settingsRetryCount :: ! Int } -- - Bad data Settings = Settings { settingsHasTravis :: Bool , settingsConfigPath :: FilePath , settingsRetryCount :: Int } Deriving \u00b6 Type classes in the deriving section should always be surrounded by parentheses. Don't derive typeclasses unnecessarily. Use -XDerivingStrategies extension for newtype s to explicitly specify the way you want to derive type classes: {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DerivingStrategies #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Id a = Id { unId :: Int } deriving stock ( Generic ) deriving newtype ( Eq , Ord , Show , Hashable ) deriving anyclass ( FromJSON , ToJSON ) Function declaration \u00b6 All top-level functions must have type signatures. All functions inside a where block must have type signatures. Explicit type signatures help to avoid cryptic type errors. You might need the -XScopedTypeVariables extension to write the polymorphic types of functions inside a where block. Surround . after forall in type signatures with spaces. lookup :: forall a f . Typeable a => TypeRepMap f -> Maybe ( f a ) If the function type signature is very long, then place the type of each argument under its own line with respect to alignment. sendEmail :: forall env m . ( MonadLog m , MonadEmail m , WithDb env m ) => Email -> Subject -> Body -> Template -> m () If the line with argument names is too big, then put each argument on its own line and separate it somehow from the body section. sendEmail toEmail subject @ ( Subject subj ) body Template { .. } -- default body variables = do < code goes here > In other cases, place an = sign on the same line where the function definition is. Put operator fixity before operator signature: -- | Flipped version of '<$>'. infixl 1 <&> ( <&> ) :: Functor f => f a -> ( a -> b ) -> f b as <&> f = f <$> as Put pragmas immediately following the function they apply to. -- | Lifted version of 'T.putStrLn'. putTextLn :: MonadIO m => Text -> m () putTextLn = liftIO . Text . putStrLn {-# INLINE putTextLn #-} {-# SPECIALIZE putTextLn :: Text -> IO () #-} In case of data type definitions, you must put the pragma before the type it applies to. Example: data TypeRepMap ( f :: k -> Type ) = TypeRepMap { fingerprintAs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , fingerprintBs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , trAnys :: {-# UNPACK #-} ! ( Array Any ) , trKeys :: {-# UNPACK #-} ! ( Array Any ) } if-then-else clauses \u00b6 Prefer guards over if-then-else where possible. -- + Good showParity :: Int -> Bool showParity n | even n = \"even\" | otherwise = \"odd\" -- - Meh showParity :: Int -> Bool showParity n = if even n then \"even\" else \"odd\" In the code outside do -blocks you can align if-then-else clauses like you would normal expressions: shiftInts :: [ Int ] -> [ Int ] shiftInts = map $ \\ n -> if even n then n + 1 else n - 1 Case expressions \u00b6 Align the -> arrows in the alternatives when it helps readability. -- + Good firstOrDefault :: [ a ] -> a -> a firstOrDefault list def = case list of [] -> def x : _ -> x -- - Bad foo :: IO () foo = getArgs >>= \\ case [] -> do putStrLn \"No arguments provided\" runWithNoArgs firstArg : secondArg : rest -> do putStrLn $ \"The first argument is \" ++ firstArg putStrLn $ \"The second argument is \" ++ secondArg _ -> pure () Use the -XLambdaCase extension when you perform pattern matching over the last argument of the function: fromMaybe :: a -> Maybe a -> a fromMaybe v = \\ case Nothing -> v Just x -> x let expressions \u00b6 Write every let -binding on a new line: isLimitedBy :: Integer -> Natural -> Bool isLimitedBy n limit = let intLimit = toInteger limit in n <= intLimit Put a let before each variable inside a do block. General recommendations \u00b6 Try to split code into separate modules. Avoid abusing point-free style. Sometimes code is clearer when not written in point-free style: -- + Good foo :: Int -> a -> Int foo n x = length $ replicate n x -- - Bad foo :: Int -> a -> Int foo = ( length . ) . replicate Code should be compilable with the following ghc options without warnings: -Wall -Wincomplete-uni-patterns -Wincomplete-record-updates -Wcompat -Widentities -Wredundant-constraints -Wmissing-export-lists -Wpartial-fields Enable -fhide-source-paths and -freverse-errors for cleaner compiler output. Use -XApplicativeDo in combination with -XRecordWildCards to prevent position-sensitive errors where possible.","title":"Haskell style"},{"location":"contributing/haskell-style/#haskell-style-guide","text":"This document is a slightly modified version of style guide used in Kowainik .","title":"Haskell Style Guide"},{"location":"contributing/haskell-style/#style-guide-goals","text":"The purpose of this document is to help developers and people working on Haskell code-bases to have a smoother experience while dealing with code in different situations. This style guide aims to increase productivity by defining the following goals: Make code easier to understand: ideas for solutions should not be hidden behind complex and obscure code. Make code easier to read: code arrangement should be immediately apparent after looking at the existing code. Names of functions & variables should be transparent and obvious. Make code easier to write: developers should think about code formatting rules as little as possible. The style guide should answer any query pertaining to the formatting of a specific piece of code. Make code easier to maintain: this style guide aims to reduce the burden of maintaining packages using version control systems unless this conflicts with the previous points. Rule of thumb when working with existing source code The general rule is to stick to the same coding style that is already used in the file you are editing. If you must make significant style modifications, then commit them independently from the functional changes so that someone looking back through the changelog can easily distinguish between them.","title":"Style guide goals"},{"location":"contributing/haskell-style/#indentation","text":"Indent code blocks with 2 spaces . Always put a where keyword on a new line. showSign :: Int -> String showSign n | n == 0 = \"Zero\" | n < 0 = \"Negative\" | otherwise = \"Positive\" greet :: IO () greet = do putStrLn \"What is your name?\" name <- getLine putStrLn $ greeting name where greeting :: String -> String greeting name = \"Hey \" ++ name ++ \"!\"","title":"Indentation"},{"location":"contributing/haskell-style/#line-length","text":"The maximum preferred line length is 80 characters . Tip There is no hard rules when it comes to line length. Some lines just have to be a bit longer than usual. However, if your line of code exceeds this limit, try to split code into smaller chunks or break long lines over multiple shorter ones as much as you can.","title":"Line length"},{"location":"contributing/haskell-style/#whitespaces","text":"No trailing whitespaces (use some tools to automatically cleanup trailing whitespaces). Surround binary operators with a single space on either side.","title":"Whitespaces"},{"location":"contributing/haskell-style/#alignment","text":"Use comma-leading style for formatting module exports, lists, tuples, records, etc. answers :: [ Maybe Int ] answers = [ Just 42 , Just 7 , Nothing ] If a function definition doesn't fit the line limit then align multiple lines according to the same separator like :: , => , -> . -- + Good printQuestion :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () -- + Acceptable if function name is short fun :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () Align records with every field on a separate line with leading commas. -- + Good data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) -- + Acceptable data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) Align sum types with every constructor on its own line with leading = and | . -- + Good data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) -- + Acceptable data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) Try to follow the above rule inside function definitions but without fanatism: -- + Good createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo -- there's no need to put the constructor on a separate line and have an extra line <$> veryLongBar <*> veryLongBaz Basically, it is often possible to join consequent lines without introducing alignment dependency. Try not to span multiple short lines unnecessarily. If a function application must spawn multiple lines to fit within the maximum line length, then write one argument on each line following the head, indented by one level: veryLongProductionName firstArgumentOfThisFunction secondArgumentOfThisFunction ( DummyDatatype withDummyField1 andDummyField2 ) lastArgumentOfThisFunction","title":"Alignment"},{"location":"contributing/haskell-style/#naming","text":"","title":"Naming"},{"location":"contributing/haskell-style/#functions-and-variables","text":"lowerCamelCase for function and variable names. UpperCamelCase for data types, typeclasses and constructors. Variant Use ids_with_underscores for local variables only. Try not to create new operators. -- What does this 'mouse operator' mean? :thinking_suicide: ( ~@@^> ) :: Functor f => ( a -> b ) -> ( a -> c -> d ) -> ( b -> f c ) -> a -> f d Do not use ultra-short or indescriptive names like a , par , g unless the types of these variables are general enough. -- + Good mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect test ifTrue ifFalse = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if test x then ifTrue x : go xs else ifFalse x : go xs -- - Bad mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect p f g = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if p x then f x : go xs else g x : go xs Do not introduce unnecessarily long names for variables. -- + Good map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map f ( x : xs ) = f x : map f xs -- - Bad map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map function ( firstElement : remainingList ) = function firstElement : map function remainingList For readability reasons, do not capitalize all letters when using an abbreviation as a part of a longer name. For example, write TomlException instead of TOMLException . Unicode symbols are allowed only in modules that already use unicode symbols. If you create a unicode name, you should also create a non-unicode one as an alias.","title":"Functions and variables"},{"location":"contributing/haskell-style/#data-types","text":"Creating data types is extremely easy in Haskell. It is usually a good idea to introduce a custom data type (enum or newtype ) instead of using a commonly used data type (like Int , String , Set Text , etc.). type aliases are allowed only for specializing general types: -- + Good data StateT s m a type State s = StateT s Identity -- - Bad type Size = Int Use the data type name as the constructor name for data with single constructor and newtype . data User = User Int String The field name for a newtype must be prefixed by un followed by the type name. newtype Size = Size { unSize :: Int } newtype App a = App { unApp :: ReaderT Context IO a } Field names for the record data type should start with the full name of the data type. -- + Good data HealthReading = HealthReading { healthReadingDate :: UTCTime , healthReadingMeasurement :: Double } It is acceptable to use an abbreviation as the field prefix if the data type name is too long. -- + Acceptable data HealthReading = HealthReading { hrDate :: UTCTime , hrMeasurement :: Double }","title":"Data types"},{"location":"contributing/haskell-style/#comments","text":"Separate end-of-line comments from the code with 2 spaces . newtype Measure = Measure { unMeasure :: Double -- ^ See how 2 spaces separate this comment } Write Haddock documentation for the top-level functions, function arguments and data type fields. The documentation should give enough information to apply the function without looking at its definition. -- | Single-line short comment. foo :: Int -> [ a ] -> [ a ] -- | Example of multi-line block comment which is very long -- and doesn't fit single line. foo :: Int -> [ a ] -> [ a ] -- + Good -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list -> a -- ^ Element to populate list -> [ a ] -- - Bad -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list {- | Element to populate list -} -> a -> [ a ] If possible, include typeclass laws and function usage examples into the documentation. -- | The class of semigroups (types with an associative binary operation). -- -- Instances should satisfy the associativity law: -- -- * @x '<>' (y '<>' z) = (x '<>' y) '<>' z@ class Semigroup a where ( <> ) :: a -> a -> a -- | The 'intersperse' function takes a character and places it -- between the characters of a 'Text'. -- -- >>> T.intersperse '.' \"SHIELD\" -- \"S.H.I.E.L.D\" intersperse :: Char -> Text -> Text","title":"Comments"},{"location":"contributing/haskell-style/#guideline-for-module-formatting","text":"Allowed tools for automatic module formatting: stylish-haskell : for formatting the import section and for alignment.","title":"Guideline for module formatting"},{"location":"contributing/haskell-style/#language","text":"Put OPTIONS_GHC pragma before LANGUAGE pragmas in a separate section. Write each LANGUAGE pragma on its own line, sort them alphabetically and align by max width among them. {-# OPTIONS_GHC -fno-warn-orphans #-} {-# LANGUAGE ApplicativeDo #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeApplications #-} Always put language extensions in the relevant source file. Tip Language extensions must be listed at the very top of the file, above the module name.","title":"LANGUAGE"},{"location":"contributing/haskell-style/#export-lists","text":"Use the following rules to format the export section: Always write an explicit export list. Indent the export list by 2 spaces . You can split the export list into sections. Use Haddock to assign names to these sections. Classes, data types and type aliases should be written before functions in each section. module Map ( -- * Data type Map , Key , empty -- * Update , insert , insertWith , alter ) where","title":"Export lists"},{"location":"contributing/haskell-style/#imports","text":"Always use explicit import lists or qualified imports. Use qualified imports only if the import list is big enough or there are conflicts in names. This makes the code more robust against changes in dependent libraries. Exception: modules that only reexport other entire modules. Imports should be grouped in the following order: Imports from Hackage packages. Imports from the current project. Put a blank line between each group of imports. The imports in each group should be sorted alphabetically by module name. module MyProject.Foo ( Foo ( .. ) ) where import Control.Exception ( catch , try ) import qualified Data.Aeson as Json import qualified Data.Text as Text import Data.Traversable ( for ) import MyProject.Ansi ( errorMessage , infoMessage ) import qualified MyProject.BigModule as Big data Foo ...","title":"Imports"},{"location":"contributing/haskell-style/#data-declaration","text":"Refer to the Alignment section to see how to format data type declarations. Records for data types with multiple constructors are forbidden. -- - Bad data Foo = Bar { bar1 :: Int , bar2 :: Double } | Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Good data Foo = FooBar Bar | FooBaz Baz data Bar = Bar { bar1 :: Int , bar2 :: Double } data Baz = Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Also good data Foo = Bar Int Double | Baz Int Double Text","title":"Data declaration"},{"location":"contributing/haskell-style/#strictness","text":"Fields of data type constructors should be strict. Specify strictness explicitly with ! . This helps to avoid space leaks and gives you an error instead of a warning in case you forget to initialize some fields. -- + Good data Settings = Settings { settingsHasTravis :: ! Bool , settingsConfigPath :: ! FilePath , settingsRetryCount :: ! Int } -- - Bad data Settings = Settings { settingsHasTravis :: Bool , settingsConfigPath :: FilePath , settingsRetryCount :: Int }","title":"Strictness"},{"location":"contributing/haskell-style/#deriving","text":"Type classes in the deriving section should always be surrounded by parentheses. Don't derive typeclasses unnecessarily. Use -XDerivingStrategies extension for newtype s to explicitly specify the way you want to derive type classes: {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DerivingStrategies #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Id a = Id { unId :: Int } deriving stock ( Generic ) deriving newtype ( Eq , Ord , Show , Hashable ) deriving anyclass ( FromJSON , ToJSON )","title":"Deriving"},{"location":"contributing/haskell-style/#function-declaration","text":"All top-level functions must have type signatures. All functions inside a where block must have type signatures. Explicit type signatures help to avoid cryptic type errors. You might need the -XScopedTypeVariables extension to write the polymorphic types of functions inside a where block. Surround . after forall in type signatures with spaces. lookup :: forall a f . Typeable a => TypeRepMap f -> Maybe ( f a ) If the function type signature is very long, then place the type of each argument under its own line with respect to alignment. sendEmail :: forall env m . ( MonadLog m , MonadEmail m , WithDb env m ) => Email -> Subject -> Body -> Template -> m () If the line with argument names is too big, then put each argument on its own line and separate it somehow from the body section. sendEmail toEmail subject @ ( Subject subj ) body Template { .. } -- default body variables = do < code goes here > In other cases, place an = sign on the same line where the function definition is. Put operator fixity before operator signature: -- | Flipped version of '<$>'. infixl 1 <&> ( <&> ) :: Functor f => f a -> ( a -> b ) -> f b as <&> f = f <$> as Put pragmas immediately following the function they apply to. -- | Lifted version of 'T.putStrLn'. putTextLn :: MonadIO m => Text -> m () putTextLn = liftIO . Text . putStrLn {-# INLINE putTextLn #-} {-# SPECIALIZE putTextLn :: Text -> IO () #-} In case of data type definitions, you must put the pragma before the type it applies to. Example: data TypeRepMap ( f :: k -> Type ) = TypeRepMap { fingerprintAs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , fingerprintBs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , trAnys :: {-# UNPACK #-} ! ( Array Any ) , trKeys :: {-# UNPACK #-} ! ( Array Any ) }","title":"Function declaration"},{"location":"contributing/haskell-style/#if-then-else-clauses","text":"Prefer guards over if-then-else where possible. -- + Good showParity :: Int -> Bool showParity n | even n = \"even\" | otherwise = \"odd\" -- - Meh showParity :: Int -> Bool showParity n = if even n then \"even\" else \"odd\" In the code outside do -blocks you can align if-then-else clauses like you would normal expressions: shiftInts :: [ Int ] -> [ Int ] shiftInts = map $ \\ n -> if even n then n + 1 else n - 1","title":"if-then-else clauses"},{"location":"contributing/haskell-style/#case-expressions","text":"Align the -> arrows in the alternatives when it helps readability. -- + Good firstOrDefault :: [ a ] -> a -> a firstOrDefault list def = case list of [] -> def x : _ -> x -- - Bad foo :: IO () foo = getArgs >>= \\ case [] -> do putStrLn \"No arguments provided\" runWithNoArgs firstArg : secondArg : rest -> do putStrLn $ \"The first argument is \" ++ firstArg putStrLn $ \"The second argument is \" ++ secondArg _ -> pure () Use the -XLambdaCase extension when you perform pattern matching over the last argument of the function: fromMaybe :: a -> Maybe a -> a fromMaybe v = \\ case Nothing -> v Just x -> x","title":"Case expressions"},{"location":"contributing/haskell-style/#let-expressions","text":"Write every let -binding on a new line: isLimitedBy :: Integer -> Natural -> Bool isLimitedBy n limit = let intLimit = toInteger limit in n <= intLimit Put a let before each variable inside a do block.","title":"let expressions"},{"location":"contributing/haskell-style/#general-recommendations","text":"Try to split code into separate modules. Avoid abusing point-free style. Sometimes code is clearer when not written in point-free style: -- + Good foo :: Int -> a -> Int foo n x = length $ replicate n x -- - Bad foo :: Int -> a -> Int foo = ( length . ) . replicate Code should be compilable with the following ghc options without warnings: -Wall -Wincomplete-uni-patterns -Wincomplete-record-updates -Wcompat -Widentities -Wredundant-constraints -Wmissing-export-lists -Wpartial-fields Enable -fhide-source-paths and -freverse-errors for cleaner compiler output. Use -XApplicativeDo in combination with -XRecordWildCards to prevent position-sensitive errors where possible.","title":"General recommendations"},{"location":"deployment/deploy-k8s/","text":"Running on Kubernetes \u00b6 This document describes how to run HStreamDB kubernetes using the specs that we provide. The document assumes basic previous kubernetes knowledge. By the end of this section, you'll have a fully running HStreamDB cluster on kubernetes that's ready to receive reads/writes, process datas, etc. Building your Kubernetes Cluster \u00b6 The first step is to have a running kubernetes cluster. You can use a managed cluster (provided by your cloud provider), a self-hosted cluster or a local kubernetes cluster using a tool like minikube. Make sure that kubectl points to whatever cluster you're planning to use. Also, you need a storageClass named hstream-store , you can create by kubectl or by your cloud provider web page if it has. Install Zookeeper \u00b6 HStreamDB depends on Zookeeper for storing queries information and some internal storage configuration. So we will need to provision a zookeeper ensemble that HStreamDB will be able to access. For this demo, we will use helm (A package manager for kubernetes) to install zookeeper. After installing helm run: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install zookeeper bitnami/zookeeper \\ --set image.tag = 3 .6.3 \\ --set replicaCount = 3 \\ --set persistence.storageClass = hstream-store \\ --set persistence.size = 20Gi NAME: zookeeper LAST DEPLOYED: Tue Jul 6 10:51:37 2021 NAMESPACE: test STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster: zookeeper.svc.cluster.local To connect to your ZooKeeper server run the following commands: export POD_NAME=$(kubectl get pods -l \"app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl exec -it $POD_NAME -- zkCli.sh To connect to your ZooKeeper server from outside the cluster execute the following commands: kubectl port-forward svc/zookeeper 2181:2181 & zkCli.sh 127.0.0.1:2181 WARNING: Rolling tag detected (bitnami/zookeeper:3.6.3), please note that it is strongly recommended to avoid using rolling tags in a production environment. +info https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/ This will by default install a 3 nodes zookeeper ensemble. Wait until all the three pods are marked as ready: kubectl get pods NAME READY STATUS RESTARTS AGE zookeeper-0 1/1 Running 0 22h zookeeper-1 1/1 Running 0 4d22h zookeeper-2 1/1 Running 0 16m Configuring and Starting HStreamDB \u00b6 Once all the zookeeper pods are ready, we're ready to start installing the HStreamDB cluster. Fetching The K8s Specs \u00b6 git clone git@github.com:hstreamdb/hstream.git cd hstream/k8s Update Configuration \u00b6 If you used a different way to install zookeeper, make sure to update the zookeeper connection string in storage config file config.json and server service file hstream-server.yaml . It should look something like this: $ cat config.json | grep -A 2 zookeeper \"zookeeper\" : { \"zookeeper_uri\" : \"ip://zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" , \"timeout\" : \"30s\" } $ cat hstream-server.yaml | grep -A 1 zkuri - \"--zkuri\" - \"zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" Tips The zookeeper connection string in stotage config file and the service file can be different. But for normal scenario, they are the same. By default, this spec installs a 3 nodes HStream server cluster and 4 nodes storage cluster. If you want a bigger cluster, modify the hstream-server.yaml and logdevice-statefulset.yaml file, and increase the number of replicas to the number of nodes you want in the cluster. Also by default, we attach a 40GB persistent storage to the nodes, if you want more you can change that under the volumeClaimTemplates section. Starting the Cluster \u00b6 kubectl apply -k . When you run kubectl get pods , you should see something like this: NAME READY STATUS RESTARTS AGE hstream-server-deployment-765c84c489-94nqd 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jrm5p 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jxsjd 1/1 Running 0 6d18h logdevice-0 1/1 Running 0 6d18h logdevice-1 1/1 Running 0 6d18h logdevice-2 1/1 Running 0 6d18h logdevice-3 1/1 Running 0 6d18h logdevice-admin-server-deployment-5c5fb9f8fb-27jlk 1/1 Running 0 6d18h zookeeper-0 1/1 Running 0 6d22h zookeeper-1 1/1 Running 0 10d zookeeper-2 1/1 Running 0 6d Bootstrapping the Storage Cluster \u00b6 Once all the logdevice pods are running and ready, you'll need to bootstrap the cluster to enable all the nodes. To do that, run: kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- \\ hadmin --host logdevice-admin-server-service \\ nodes-config \\ bootstrap --metadata-replicate-across 'node:3' This will start a hadmin pod, that connects to the admin server and invokes the nodes-config bootstrap hadmin command and sets the metadata replication property of the cluster to be replicated across three different nodes. On success, you should see something like: Successfully bootstrapped the cluster pod \"hadmin\" deleted Managing the Storage Cluster \u00b6 kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- bash Now you can run hadmin to manage the cluster: hadmin --help To check the state of the cluster, you can then run: hadmin --host logdevice-admin-server-service status +----+-------------+-------+---------------+ | ID | NAME | STATE | HEALTH STATUS | +----+-------------+-------+---------------+ | 0 | logdevice-0 | ALIVE | HEALTHY | | 1 | logdevice-1 | ALIVE | HEALTHY | | 2 | logdevice-2 | ALIVE | HEALTHY | | 3 | logdevice-3 | ALIVE | HEALTHY | +----+-------------+-------+---------------+ Took 2.567s","title":"Running on Kubernetes"},{"location":"deployment/deploy-k8s/#running-on-kubernetes","text":"This document describes how to run HStreamDB kubernetes using the specs that we provide. The document assumes basic previous kubernetes knowledge. By the end of this section, you'll have a fully running HStreamDB cluster on kubernetes that's ready to receive reads/writes, process datas, etc.","title":"Running on Kubernetes"},{"location":"deployment/deploy-k8s/#building-your-kubernetes-cluster","text":"The first step is to have a running kubernetes cluster. You can use a managed cluster (provided by your cloud provider), a self-hosted cluster or a local kubernetes cluster using a tool like minikube. Make sure that kubectl points to whatever cluster you're planning to use. Also, you need a storageClass named hstream-store , you can create by kubectl or by your cloud provider web page if it has.","title":"Building your Kubernetes Cluster"},{"location":"deployment/deploy-k8s/#install-zookeeper","text":"HStreamDB depends on Zookeeper for storing queries information and some internal storage configuration. So we will need to provision a zookeeper ensemble that HStreamDB will be able to access. For this demo, we will use helm (A package manager for kubernetes) to install zookeeper. After installing helm run: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install zookeeper bitnami/zookeeper \\ --set image.tag = 3 .6.3 \\ --set replicaCount = 3 \\ --set persistence.storageClass = hstream-store \\ --set persistence.size = 20Gi NAME: zookeeper LAST DEPLOYED: Tue Jul 6 10:51:37 2021 NAMESPACE: test STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster: zookeeper.svc.cluster.local To connect to your ZooKeeper server run the following commands: export POD_NAME=$(kubectl get pods -l \"app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl exec -it $POD_NAME -- zkCli.sh To connect to your ZooKeeper server from outside the cluster execute the following commands: kubectl port-forward svc/zookeeper 2181:2181 & zkCli.sh 127.0.0.1:2181 WARNING: Rolling tag detected (bitnami/zookeeper:3.6.3), please note that it is strongly recommended to avoid using rolling tags in a production environment. +info https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/ This will by default install a 3 nodes zookeeper ensemble. Wait until all the three pods are marked as ready: kubectl get pods NAME READY STATUS RESTARTS AGE zookeeper-0 1/1 Running 0 22h zookeeper-1 1/1 Running 0 4d22h zookeeper-2 1/1 Running 0 16m","title":"Install Zookeeper"},{"location":"deployment/deploy-k8s/#configuring-and-starting-hstreamdb","text":"Once all the zookeeper pods are ready, we're ready to start installing the HStreamDB cluster.","title":"Configuring and Starting HStreamDB"},{"location":"deployment/deploy-k8s/#fetching-the-k8s-specs","text":"git clone git@github.com:hstreamdb/hstream.git cd hstream/k8s","title":"Fetching The K8s Specs"},{"location":"deployment/deploy-k8s/#update-configuration","text":"If you used a different way to install zookeeper, make sure to update the zookeeper connection string in storage config file config.json and server service file hstream-server.yaml . It should look something like this: $ cat config.json | grep -A 2 zookeeper \"zookeeper\" : { \"zookeeper_uri\" : \"ip://zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" , \"timeout\" : \"30s\" } $ cat hstream-server.yaml | grep -A 1 zkuri - \"--zkuri\" - \"zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" Tips The zookeeper connection string in stotage config file and the service file can be different. But for normal scenario, they are the same. By default, this spec installs a 3 nodes HStream server cluster and 4 nodes storage cluster. If you want a bigger cluster, modify the hstream-server.yaml and logdevice-statefulset.yaml file, and increase the number of replicas to the number of nodes you want in the cluster. Also by default, we attach a 40GB persistent storage to the nodes, if you want more you can change that under the volumeClaimTemplates section.","title":"Update Configuration"},{"location":"deployment/deploy-k8s/#starting-the-cluster","text":"kubectl apply -k . When you run kubectl get pods , you should see something like this: NAME READY STATUS RESTARTS AGE hstream-server-deployment-765c84c489-94nqd 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jrm5p 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jxsjd 1/1 Running 0 6d18h logdevice-0 1/1 Running 0 6d18h logdevice-1 1/1 Running 0 6d18h logdevice-2 1/1 Running 0 6d18h logdevice-3 1/1 Running 0 6d18h logdevice-admin-server-deployment-5c5fb9f8fb-27jlk 1/1 Running 0 6d18h zookeeper-0 1/1 Running 0 6d22h zookeeper-1 1/1 Running 0 10d zookeeper-2 1/1 Running 0 6d","title":"Starting the Cluster"},{"location":"deployment/deploy-k8s/#bootstrapping-the-storage-cluster","text":"Once all the logdevice pods are running and ready, you'll need to bootstrap the cluster to enable all the nodes. To do that, run: kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- \\ hadmin --host logdevice-admin-server-service \\ nodes-config \\ bootstrap --metadata-replicate-across 'node:3' This will start a hadmin pod, that connects to the admin server and invokes the nodes-config bootstrap hadmin command and sets the metadata replication property of the cluster to be replicated across three different nodes. On success, you should see something like: Successfully bootstrapped the cluster pod \"hadmin\" deleted","title":"Bootstrapping the Storage Cluster"},{"location":"deployment/deploy-k8s/#managing-the-storage-cluster","text":"kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- bash Now you can run hadmin to manage the cluster: hadmin --help To check the state of the cluster, you can then run: hadmin --host logdevice-admin-server-service status +----+-------------+-------+---------------+ | ID | NAME | STATE | HEALTH STATUS | +----+-------------+-------+---------------+ | 0 | logdevice-0 | ALIVE | HEALTHY | | 1 | logdevice-1 | ALIVE | HEALTHY | | 2 | logdevice-2 | ALIVE | HEALTHY | | 3 | logdevice-3 | ALIVE | HEALTHY | +----+-------------+-------+---------------+ Took 2.567s","title":"Managing the Storage Cluster"},{"location":"develop/cli/cli/","text":"TODO \u00b6","title":"CLI Usage"},{"location":"develop/cli/cli/#todo","text":"","title":"TODO"},{"location":"develop/java-sdk/connect/","text":"Connect \u00b6 This page shows how to connect to HStreamDB using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Example \u00b6 package io.hstream.example ; import io.hstream.HStreamClient ; public class ConnectExample { public static void main ( String [] args ) throws Exception { final String serviceUrl = \"localhost:6570\" ; HStreamClient client = HStreamClient . builder (). serviceUrl ( serviceUrl ). build (); System . out . println ( \"Connected\" ); client . close (); } }","title":"Connect"},{"location":"develop/java-sdk/connect/#connect","text":"This page shows how to connect to HStreamDB using Java SDK.","title":"Connect"},{"location":"develop/java-sdk/connect/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/connect/#example","text":"package io.hstream.example ; import io.hstream.HStreamClient ; public class ConnectExample { public static void main ( String [] args ) throws Exception { final String serviceUrl = \"localhost:6570\" ; HStreamClient client = HStreamClient . builder (). serviceUrl ( serviceUrl ). build (); System . out . println ( \"Connected\" ); client . close (); } }","title":"Example"},{"location":"develop/java-sdk/consume/","text":"Consume data \u00b6 This page shows how to consume data from HStreamDB using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Consumer \u00b6 Before you can consume data, you first need to create a Consumer object using the HStreamClient.newConsumer() method: Consumer consumer = client . newConsumer () . subscription ( \"test_subscription\" ) . stream ( \"test_stream\" ) . maxPollRecords ( 100 ) . pollTimeoutMs ( 1000 ) . build (); A consumer must be associated with a subscription, and a subscription contains a stream. Once the consumer is created successfully, it can be used to continuously receive data from the subscribed stream. Receive Raw Records \u00b6 You can receive receive raw records using the Consumer.pollRawRecords() method: while ( true ) { List < ReceivedRawRecord > receivedRawRecords = consumer . pollRawRecords (); for ( ReceivedRawRecord receivedRawRecord : receivedRawRecords ) { System . out . println ( receivedRawRecord . getRecordId ()); } } Receive HRecords \u00b6 You can receive receive hrecords using the Consumer.pollHRecords() method: while ( true ) { List < ReceivedHRecord > receivedHRecords = consumer . pollHRecords (); for ( ReceivedHRecord receivedHRecord : receivedHRecords ) { System . out . println ( receivedHRecord . getRecordId ()); } }","title":"Consume Data"},{"location":"develop/java-sdk/consume/#consume-data","text":"This page shows how to consume data from HStreamDB using Java SDK.","title":"Consume data"},{"location":"develop/java-sdk/consume/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/consume/#consumer","text":"Before you can consume data, you first need to create a Consumer object using the HStreamClient.newConsumer() method: Consumer consumer = client . newConsumer () . subscription ( \"test_subscription\" ) . stream ( \"test_stream\" ) . maxPollRecords ( 100 ) . pollTimeoutMs ( 1000 ) . build (); A consumer must be associated with a subscription, and a subscription contains a stream. Once the consumer is created successfully, it can be used to continuously receive data from the subscribed stream.","title":"Consumer"},{"location":"develop/java-sdk/consume/#receive-raw-records","text":"You can receive receive raw records using the Consumer.pollRawRecords() method: while ( true ) { List < ReceivedRawRecord > receivedRawRecords = consumer . pollRawRecords (); for ( ReceivedRawRecord receivedRawRecord : receivedRawRecords ) { System . out . println ( receivedRawRecord . getRecordId ()); } }","title":"Receive Raw Records"},{"location":"develop/java-sdk/consume/#receive-hrecords","text":"You can receive receive hrecords using the Consumer.pollHRecords() method: while ( true ) { List < ReceivedHRecord > receivedHRecords = consumer . pollHRecords (); for ( ReceivedHRecord receivedHRecord : receivedHRecords ) { System . out . println ( receivedHRecord . getRecordId ()); } }","title":"Receive HRecords"},{"location":"develop/java-sdk/installation/","text":"Installation \u00b6 The HStreamDB Java SDK is published in Maven central, available at hstreamdb-java . Maven \u00b6 For Maven Users, the library can be included easily like this: <dependencies> <dependency> <groupId> io.hstream </groupId> <artifactId> hstreamdb-java </artifactId> <version> ${hstreamdbClient.version} </version> </dependency> </dependencies> Gradle \u00b6 For Gradle Users, the library can be included easily like this: compile group: 'io.hstreamdb' , name: 'hstreamdb-java' , version: \"${hstreamdbClientVersion}\"","title":"Installation"},{"location":"develop/java-sdk/installation/#installation","text":"The HStreamDB Java SDK is published in Maven central, available at hstreamdb-java .","title":"Installation"},{"location":"develop/java-sdk/installation/#maven","text":"For Maven Users, the library can be included easily like this: <dependencies> <dependency> <groupId> io.hstream </groupId> <artifactId> hstreamdb-java </artifactId> <version> ${hstreamdbClient.version} </version> </dependency> </dependencies>","title":"Maven"},{"location":"develop/java-sdk/installation/#gradle","text":"For Gradle Users, the library can be included easily like this: compile group: 'io.hstreamdb' , name: 'hstreamdb-java' , version: \"${hstreamdbClientVersion}\"","title":"Gradle"},{"location":"develop/java-sdk/query/","text":"Stream Processing with SQL \u00b6 This page shows how to processing stream data in HStreamDB with SQL using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Execute Real-time Query on Stream \u00b6 You can execute a real-time query on stream using the HStreamClient.streamQuery() method: final String TEST_STREAM = \"test_stream\" ; Publisher < HRecord > publisher = client . streamQuery ( \"select * from \" + TEST_STREAM + \" where temperature > 30 emit changes;\" ); Observer < HRecord > observer = new Observer < HRecord > () { @Override public void onNext ( HRecord hrecord ) { System . out . println ( hrecord ); } @Override public void onError ( Throwable t ) { throw new RuntimeException ( t ); } @Override public void onCompleted () { } }; publisher . subscribe ( observer ); The HStreamClient.streamQuery() method return a Publisher object, and you need to provide an Observer object that contains your logic for processing the results returned by the query.","title":"Stream Processing with SQL"},{"location":"develop/java-sdk/query/#stream-processing-with-sql","text":"This page shows how to processing stream data in HStreamDB with SQL using Java SDK.","title":"Stream Processing with SQL"},{"location":"develop/java-sdk/query/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/query/#execute-real-time-query-on-stream","text":"You can execute a real-time query on stream using the HStreamClient.streamQuery() method: final String TEST_STREAM = \"test_stream\" ; Publisher < HRecord > publisher = client . streamQuery ( \"select * from \" + TEST_STREAM + \" where temperature > 30 emit changes;\" ); Observer < HRecord > observer = new Observer < HRecord > () { @Override public void onNext ( HRecord hrecord ) { System . out . println ( hrecord ); } @Override public void onError ( Throwable t ) { throw new RuntimeException ( t ); } @Override public void onCompleted () { } }; publisher . subscribe ( observer ); The HStreamClient.streamQuery() method return a Publisher object, and you need to provide an Observer object that contains your logic for processing the results returned by the query.","title":"Execute Real-time Query on Stream"},{"location":"develop/java-sdk/stream/","text":"Streams \u00b6 HStreamDB stores data in streams, and this page shows how to operate streams using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Include following import statements: import io.hstream.HStreamClient ; import io.hstream.Stream ; Connect to a HStreamDB Instance \u00b6 First, you need to connect to a HStreaDB instance and get a HStreamClient oject. HStreamClient client = HStreamClient . builder (). serviceUrl ( \"localhost:6570).build(); Get a List of Streams \u00b6 You can get a list of the streams using the HStreamClient.listStreams() method: for ( Stream stream : client . listStreams ()) { System . out . println ( stream . getStreamName ()); } Create a New Stream \u00b6 You can create a new stream using the HStreamClient.createStream() method: client . createStream ( \"test_stream\" ); Delete a Stream \u00b6 You can delete a stream using the HStreamClient.deleteStream() method: client . deleteStream ( \"test_stream\" );","title":"Streams"},{"location":"develop/java-sdk/stream/#streams","text":"HStreamDB stores data in streams, and this page shows how to operate streams using Java SDK.","title":"Streams"},{"location":"develop/java-sdk/stream/#prerequisites","text":"Make sure you have HStreamDB running and accessible. Include following import statements: import io.hstream.HStreamClient ; import io.hstream.Stream ;","title":"Prerequisites"},{"location":"develop/java-sdk/stream/#connect-to-a-hstreamdb-instance","text":"First, you need to connect to a HStreaDB instance and get a HStreamClient oject. HStreamClient client = HStreamClient . builder (). serviceUrl ( \"localhost:6570).build();","title":"Connect to a HStreamDB Instance"},{"location":"develop/java-sdk/stream/#get-a-list-of-streams","text":"You can get a list of the streams using the HStreamClient.listStreams() method: for ( Stream stream : client . listStreams ()) { System . out . println ( stream . getStreamName ()); }","title":"Get a List of Streams"},{"location":"develop/java-sdk/stream/#create-a-new-stream","text":"You can create a new stream using the HStreamClient.createStream() method: client . createStream ( \"test_stream\" );","title":"Create a New Stream"},{"location":"develop/java-sdk/stream/#delete-a-stream","text":"You can delete a stream using the HStreamClient.deleteStream() method: client . deleteStream ( \"test_stream\" );","title":"Delete a Stream"},{"location":"develop/java-sdk/write/","text":"Write data \u00b6 This page shows how to write data into HStreamDB using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Concepts \u00b6 You can write two types of data to streams in HStreamDB: raw record hstream record(HRecord) Raw Record \u00b6 Raw record represents arbitray binary data. You can save binary data to a stream, but please note that currently stream processing via sql ignores binary data, it now only processes HRecord type data. Of course, you can always get the binary data from the stream and process it yourself. HRecord \u00b6 You can think of an HRecord as a piece of JSON data\uff0c just like a document in some nosql databases. You can process hrecords directly in real time via sql statements. Producer \u00b6 Before you can write data, you first need to create a Producer object using the HStreamClient.newProducer() method: Producer producer = client . newProducer (). stream ( \"test_stream\" ). build (); A producer has some options, for now, let's just ignore them and use the default settings. Write Binary Data \u00b6 Write Binary Data Synchronously \u00b6 You can write binary data synchronously using the Producer.write() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); RecordId recordId = producer . write ( rawRecord ); Write Binary Data Asynchronously \u00b6 You can write binary data asynchronously using the Producer.writeAsync() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord ); Write HRecord \u00b6 Write HRecord Synchronously \u00b6 You can write hrecords synchronously using the Producer.write() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); RecordId recordId = producer . write ( hRecord ); Write HRecord Asynchronously \u00b6 You can write hrecords asynchronously using the Producer.writeAsync() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); CompletableFuture < RecordId > future = producer . write ( hRecord ); Buffered Writes (Preferred) \u00b6 When writing to HStreamDB, sending many small records limits throughput. To achieve higher thoughput, you can enable batch mode of Producer . Producer producer = client . newProducer () . stream ( \"test_stream\" ) . enableBatch () . recordCountLimit ( 100 ) . build (); Then you can still write data using the Producer.writeAsync() Random random = new Random (); final int count = 1000 ; CompletableFuture < RecordId >[] recordIdFutures = new CompletableFuture [ count ] ; for ( int i = 0 ; i < count ; ++ i ) { byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord ); recordIdFutures [ i ] = future ; } Now the producer will first put the data submitted by the writeAsync method in an internal buffer and send it together to the HStreamDB server when the number reaches recordLimitCount , or you can call flush method manually at any time to flush the buffer. producer . flush (); Warnings \u00b6 Please do not write both binary data and hrecord in one stream.","title":"Write Data"},{"location":"develop/java-sdk/write/#write-data","text":"This page shows how to write data into HStreamDB using Java SDK.","title":"Write data"},{"location":"develop/java-sdk/write/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/write/#concepts","text":"You can write two types of data to streams in HStreamDB: raw record hstream record(HRecord)","title":"Concepts"},{"location":"develop/java-sdk/write/#raw-record","text":"Raw record represents arbitray binary data. You can save binary data to a stream, but please note that currently stream processing via sql ignores binary data, it now only processes HRecord type data. Of course, you can always get the binary data from the stream and process it yourself.","title":"Raw Record"},{"location":"develop/java-sdk/write/#hrecord","text":"You can think of an HRecord as a piece of JSON data\uff0c just like a document in some nosql databases. You can process hrecords directly in real time via sql statements.","title":"HRecord"},{"location":"develop/java-sdk/write/#producer","text":"Before you can write data, you first need to create a Producer object using the HStreamClient.newProducer() method: Producer producer = client . newProducer (). stream ( \"test_stream\" ). build (); A producer has some options, for now, let's just ignore them and use the default settings.","title":"Producer"},{"location":"develop/java-sdk/write/#write-binary-data","text":"","title":"Write Binary Data"},{"location":"develop/java-sdk/write/#write-binary-data-synchronously","text":"You can write binary data synchronously using the Producer.write() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); RecordId recordId = producer . write ( rawRecord );","title":"Write Binary Data Synchronously"},{"location":"develop/java-sdk/write/#write-binary-data-asynchronously","text":"You can write binary data asynchronously using the Producer.writeAsync() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord );","title":"Write Binary Data Asynchronously"},{"location":"develop/java-sdk/write/#write-hrecord","text":"","title":"Write HRecord"},{"location":"develop/java-sdk/write/#write-hrecord-synchronously","text":"You can write hrecords synchronously using the Producer.write() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); RecordId recordId = producer . write ( hRecord );","title":"Write HRecord Synchronously"},{"location":"develop/java-sdk/write/#write-hrecord-asynchronously","text":"You can write hrecords asynchronously using the Producer.writeAsync() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); CompletableFuture < RecordId > future = producer . write ( hRecord );","title":"Write HRecord Asynchronously"},{"location":"develop/java-sdk/write/#buffered-writes-preferred","text":"When writing to HStreamDB, sending many small records limits throughput. To achieve higher thoughput, you can enable batch mode of Producer . Producer producer = client . newProducer () . stream ( \"test_stream\" ) . enableBatch () . recordCountLimit ( 100 ) . build (); Then you can still write data using the Producer.writeAsync() Random random = new Random (); final int count = 1000 ; CompletableFuture < RecordId >[] recordIdFutures = new CompletableFuture [ count ] ; for ( int i = 0 ; i < count ; ++ i ) { byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord ); recordIdFutures [ i ] = future ; } Now the producer will first put the data submitted by the writeAsync method in an internal buffer and send it together to the HStreamDB server when the number reaches recordLimitCount , or you can call flush method manually at any time to flush the buffer. producer . flush ();","title":"Buffered Writes (Preferred)"},{"location":"develop/java-sdk/write/#warnings","text":"Please do not write both binary data and hrecord in one stream.","title":"Warnings"},{"location":"overview/data-lifecycle/","text":"TODO \u00b6","title":"Data Lifecycle"},{"location":"overview/data-lifecycle/#todo","text":"","title":"TODO"},{"location":"overview/features/","text":"Features \u00b6 Note: The following features the milestone of HStreamDB version 1.0. Some features are under continuous development and not yet fully implemented in the current version. Please stay tuned. HStreamDB Functional architecture Streaming data processing via SQL \u00b6 HStreamDB has designed a complete processing solution based on event time. It supports basic filtering and conversion operations, aggregations by key, calculations based on various time windows, joining between data streams, and processing disordered and late messages to ensure the accuracy of calculation results. Simultaneously, the stream processing solution of HStream is highly extensible, and users can extend the interface according to their own needs. Materialized View \u00b6 HStreamDB will offer materialized view to support complex query and analysis operations on continuously updated data streams. The incremental computing engine updates the materialized view instantly according to the changes of data streams, and users can query the materialized view through SQL statements to get real-time data insights. Data Stream Management \u00b6 HStreamDB supports the creation and management of large data streams. The creation of a data stream is a very light-weight operation based on an optimized storage design. It is possible to maintain a stable read/write latency in the case of many concurrent reads and writes. Persistent storage \u00b6 HStreamDB provides low latency and reliable data stream storage. It ensures that written data messages are not lost and can be consumed repeatedly. HStreamDB replicates written data messages to multiple storage nodes for high availability and fault tolerance and supports dumping cold data to lower-cost storage services, such as object storage, distributed file storage, etc. This means the storage capacity can be infinitely scalable and achieve permanent storage of data. Schema Management of Data Streams \u00b6 HStreamDB emphasizes flexible schema support. Data streams can be schema-less or schema-ed by JSON, Avro, Protobuf, etc. It will support schema evolution and automatically manages the compatibility between multiple versions of schemas. Data streams access and distribution \u00b6 Connector deals with access and distribution of HStreamDB data. They connect to various data systems, including MQTT Broker, MySQL, ElasticSearch, Redis, etc., facilitating integration with external data systems for users. Security Mechanism \u00b6 The security will be ensured by TLS encrypted transport and OAuth and JWT based authentication and authorization mechanism. The security plug-in interface is reserved for users to extend the default security mechanisms as needed. Monitoring and O&M tools \u00b6 We will set up a web-based console with system dashboards and visual charts, enabling detailed monitoring of cluster machine status, system key indicators, etc., which make it more convenient for O&M staff to manage the cluster.","title":"Features"},{"location":"overview/features/#features","text":"Note: The following features the milestone of HStreamDB version 1.0. Some features are under continuous development and not yet fully implemented in the current version. Please stay tuned. HStreamDB Functional architecture","title":"Features"},{"location":"overview/features/#streaming-data-processing-via-sql","text":"HStreamDB has designed a complete processing solution based on event time. It supports basic filtering and conversion operations, aggregations by key, calculations based on various time windows, joining between data streams, and processing disordered and late messages to ensure the accuracy of calculation results. Simultaneously, the stream processing solution of HStream is highly extensible, and users can extend the interface according to their own needs.","title":"Streaming data processing via SQL"},{"location":"overview/features/#materialized-view","text":"HStreamDB will offer materialized view to support complex query and analysis operations on continuously updated data streams. The incremental computing engine updates the materialized view instantly according to the changes of data streams, and users can query the materialized view through SQL statements to get real-time data insights.","title":"Materialized View"},{"location":"overview/features/#data-stream-management","text":"HStreamDB supports the creation and management of large data streams. The creation of a data stream is a very light-weight operation based on an optimized storage design. It is possible to maintain a stable read/write latency in the case of many concurrent reads and writes.","title":"Data Stream Management"},{"location":"overview/features/#persistent-storage","text":"HStreamDB provides low latency and reliable data stream storage. It ensures that written data messages are not lost and can be consumed repeatedly. HStreamDB replicates written data messages to multiple storage nodes for high availability and fault tolerance and supports dumping cold data to lower-cost storage services, such as object storage, distributed file storage, etc. This means the storage capacity can be infinitely scalable and achieve permanent storage of data.","title":"Persistent storage"},{"location":"overview/features/#schema-management-of-data-streams","text":"HStreamDB emphasizes flexible schema support. Data streams can be schema-less or schema-ed by JSON, Avro, Protobuf, etc. It will support schema evolution and automatically manages the compatibility between multiple versions of schemas.","title":"Schema Management of Data Streams"},{"location":"overview/features/#data-streams-access-and-distribution","text":"Connector deals with access and distribution of HStreamDB data. They connect to various data systems, including MQTT Broker, MySQL, ElasticSearch, Redis, etc., facilitating integration with external data systems for users.","title":"Data streams access and distribution"},{"location":"overview/features/#security-mechanism","text":"The security will be ensured by TLS encrypted transport and OAuth and JWT based authentication and authorization mechanism. The security plug-in interface is reserved for users to extend the default security mechanisms as needed.","title":"Security Mechanism"},{"location":"overview/features/#monitoring-and-om-tools","text":"We will set up a web-based console with system dashboards and visual charts, enabling detailed monitoring of cluster machine status, system key indicators, etc., which make it more convenient for O&M staff to manage the cluster.","title":"Monitoring and O&amp;M tools"},{"location":"overview/learning-resouces/","text":"TODO \u00b6","title":"Learning Resources"},{"location":"overview/learning-resouces/#todo","text":"","title":"TODO"},{"location":"overview/overview/","text":"HStream Streaming Database Overview \u00b6 HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications.","title":"HStream Streaming Database Overview"},{"location":"overview/overview/#hstream-streaming-database-overview","text":"HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications.","title":"HStream Streaming Database Overview"},{"location":"overview/release-notes/","text":"TODO \u00b6","title":"Release Notes"},{"location":"overview/release-notes/#todo","text":"","title":"TODO"},{"location":"overview/architecture/hstream-server/","text":"HStream Server \u00b6 HStream Server (HSQL), the core computation component of HStreamDB, is designed to be stateless. The primary responsibility of HSQL is to support client connection management, security authentication, SQL parsing and optimization, and operations for stream computation such as task creation, scheduling, execution, management, etc. HStream Server (HSQL) top-down layered structures \u00b6 Access Layer \u00b6 It is in charge of protocol processing, connection management, security authentication, and access control for client requests. SQL layer \u00b6 To perform most stream processing and real-time analysis tasks, clients interact with HStreamDB through SQL statements. This layer is mainly responsible for compiling these SQL statements into logical data flow diagrams. Like the classic database system model, it contains two core sub-components: SQL parser and SQL optimizer. The SQL parser deals with the lexical and syntactic analysis and the compilation from SQL statements to relational algebraic expressions; the SQL optimizer will optimize the generated execution plan based on various rules and contexts. Stream Layer \u00b6 Stream layer includes the implementation of various stream processing operators, the data structures and DSL to express data flow diagrams, and the support for user-defined functions as processing operators. So, it is responsible for selecting the corresponding operator and optimization to generate the executable data flow diagram. Runtime Layer \u00b6 It is the layer responsible for executing the computation task of data flow diagrams and returning the results. The main components of the layer include task scheduler, state manager, and execution optimizer. The schedule takes care of the tasks scheduling between available computation resources, such as multiple threads of a single process, multiple processors of a single machine, and multiple machines or containers of a distributed cluster.","title":"HStream Server (HSQL)"},{"location":"overview/architecture/hstream-server/#hstream-server","text":"HStream Server (HSQL), the core computation component of HStreamDB, is designed to be stateless. The primary responsibility of HSQL is to support client connection management, security authentication, SQL parsing and optimization, and operations for stream computation such as task creation, scheduling, execution, management, etc.","title":"HStream Server"},{"location":"overview/architecture/hstream-server/#hstream-server-hsql-top-down-layered-structures","text":"","title":"HStream Server (HSQL) top-down layered structures"},{"location":"overview/architecture/hstream-server/#access-layer","text":"It is in charge of protocol processing, connection management, security authentication, and access control for client requests.","title":"Access Layer"},{"location":"overview/architecture/hstream-server/#sql-layer","text":"To perform most stream processing and real-time analysis tasks, clients interact with HStreamDB through SQL statements. This layer is mainly responsible for compiling these SQL statements into logical data flow diagrams. Like the classic database system model, it contains two core sub-components: SQL parser and SQL optimizer. The SQL parser deals with the lexical and syntactic analysis and the compilation from SQL statements to relational algebraic expressions; the SQL optimizer will optimize the generated execution plan based on various rules and contexts.","title":"SQL layer"},{"location":"overview/architecture/hstream-server/#stream-layer","text":"Stream layer includes the implementation of various stream processing operators, the data structures and DSL to express data flow diagrams, and the support for user-defined functions as processing operators. So, it is responsible for selecting the corresponding operator and optimization to generate the executable data flow diagram.","title":"Stream Layer"},{"location":"overview/architecture/hstream-server/#runtime-layer","text":"It is the layer responsible for executing the computation task of data flow diagrams and returning the results. The main components of the layer include task scheduler, state manager, and execution optimizer. The schedule takes care of the tasks scheduling between available computation resources, such as multiple threads of a single process, multiple processors of a single machine, and multiple machines or containers of a distributed cluster.","title":"Runtime Layer"},{"location":"overview/architecture/hstream-store/","text":"HStream Storage (HStore) \u00b6 HStream Storage (HStore), the core storage component of HStreamDB, is a low-latency storage component explicitly designed for streaming data. It can store large-scale real-time data in a distributed and persistent manner and seamlessly interface with large-capacity secondary storage such as S3 through the Auto-Tiering mechanism to achieve unified storage of historical and real-time data. The core storage model of HStore is a logging model that fits with streaming data. Regard data stream as an infinitely growing log, the typical operations supported include appending and reading by batches. Also, since the data stream is immutable, it generally does not support update operations. HStream Storage (HStore) consists of following layer \u00b6 Streaming Data API layer \u00b6 This layer provides the core data stream management and read/write operations, including stream creation/deletion and writing to/consuming data in the stream. In the design of HStore, data streams are not stored as actual streams. Therefore, the creation of a stream is a very light-weight operation. There is no limit to the number of streams to be created in HStore. Besides, it supports concurrent writes to numerous data streams and still maintains a stable low latency. For the characteristics of data streams, HStore provides append operation to support fast data writing. While reading from stream data, it gives a subscription-based operation and pushes any new data written to the stream to the data consumer in real time. Replicator Layer \u00b6 This layer implements the strongly consistent replication based on an optimized Flexible Paxos consensus mechanism, ensuring the fault tolerance and high availability to data, and maximizes cluster availability through a non-deterministic data distribution policy. Moreover, it supports replication groups reconfiguration online to achieve seamless cluster data balancing and horizontal scaling. Tier1 Local Storage Layer \u00b6 The layer fulfilled local persistent storage needs of data based on the optimized RocksDB storage engine, which encapsulates the access interface of streaming data and can support low-latency writing and reading a large amount of data. Tier2 Offloader Layer \u00b6 This layer provides a unified interface encapsulation for various long-term storage systems, such as HDFS, AWS S3, etc. It supports automatic offloading of historical data to these secondary storage systems and can also be accessed through a unified streaming data interface.","title":"HStream Store (HStore)"},{"location":"overview/architecture/hstream-store/#hstream-storage-hstore","text":"HStream Storage (HStore), the core storage component of HStreamDB, is a low-latency storage component explicitly designed for streaming data. It can store large-scale real-time data in a distributed and persistent manner and seamlessly interface with large-capacity secondary storage such as S3 through the Auto-Tiering mechanism to achieve unified storage of historical and real-time data. The core storage model of HStore is a logging model that fits with streaming data. Regard data stream as an infinitely growing log, the typical operations supported include appending and reading by batches. Also, since the data stream is immutable, it generally does not support update operations.","title":"HStream Storage (HStore)"},{"location":"overview/architecture/hstream-store/#hstream-storage-hstore-consists-of-following-layer","text":"","title":"HStream Storage (HStore) consists of following layer"},{"location":"overview/architecture/hstream-store/#streaming-data-api-layer","text":"This layer provides the core data stream management and read/write operations, including stream creation/deletion and writing to/consuming data in the stream. In the design of HStore, data streams are not stored as actual streams. Therefore, the creation of a stream is a very light-weight operation. There is no limit to the number of streams to be created in HStore. Besides, it supports concurrent writes to numerous data streams and still maintains a stable low latency. For the characteristics of data streams, HStore provides append operation to support fast data writing. While reading from stream data, it gives a subscription-based operation and pushes any new data written to the stream to the data consumer in real time.","title":"Streaming Data API layer"},{"location":"overview/architecture/hstream-store/#replicator-layer","text":"This layer implements the strongly consistent replication based on an optimized Flexible Paxos consensus mechanism, ensuring the fault tolerance and high availability to data, and maximizes cluster availability through a non-deterministic data distribution policy. Moreover, it supports replication groups reconfiguration online to achieve seamless cluster data balancing and horizontal scaling.","title":"Replicator Layer"},{"location":"overview/architecture/hstream-store/#tier1-local-storage-layer","text":"The layer fulfilled local persistent storage needs of data based on the optimized RocksDB storage engine, which encapsulates the access interface of streaming data and can support low-latency writing and reading a large amount of data.","title":"Tier1 Local Storage Layer"},{"location":"overview/architecture/hstream-store/#tier2-offloader-layer","text":"This layer provides a unified interface encapsulation for various long-term storage systems, such as HDFS, AWS S3, etc. It supports automatic offloading of historical data to these secondary storage systems and can also be accessed through a unified streaming data interface.","title":"Tier2 Offloader Layer"},{"location":"overview/architecture/overview/","text":"HStream Streaming Database Overview \u00b6 HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications. Architecture \u00b6 The figure below shows the overall architecture of HStreamDB. A single HStreamDB node consists of two core components, HStream Server (HSQL) and HStream Storage (HStorage). And an HStream cluster consists of several peer-to-peer HStreamDB nodes. Clients can connect to any HStreamDB node in the cluster and perform stream processing and analysis through your familiar SQL language. HStreamDB Structure Overview","title":"Overview"},{"location":"overview/architecture/overview/#hstream-streaming-database-overview","text":"HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications.","title":"HStream Streaming Database Overview"},{"location":"overview/architecture/overview/#architecture","text":"The figure below shows the overall architecture of HStreamDB. A single HStreamDB node consists of two core components, HStream Server (HSQL) and HStream Storage (HStorage). And an HStream cluster consists of several peer-to-peer HStreamDB nodes. Clients can connect to any HStreamDB node in the cluster and perform stream processing and analysis through your familiar SQL language. HStreamDB Structure Overview","title":"Architecture"},{"location":"overview/concepts/query/","text":"Query and Stream Processing \u00b6 What is a Query? \u00b6 A Query , in short, is a Stream Processing task. Stream Processing is done by Queries . A running query will manipulate/filter stream and return continuous results in real-time. With queries, real-time monitoring and response can be easily implemented. Stream Processing \u00b6 Stream Processing in our context is that we can choose to do a series of operations to the HStream streams. In HStreamDB, steam processing supports [TODO].","title":"Query and Stream Processing"},{"location":"overview/concepts/query/#query-and-stream-processing","text":"","title":"Query and Stream Processing"},{"location":"overview/concepts/query/#what-is-a-query","text":"A Query , in short, is a Stream Processing task. Stream Processing is done by Queries . A running query will manipulate/filter stream and return continuous results in real-time. With queries, real-time monitoring and response can be easily implemented.","title":"What is a Query?"},{"location":"overview/concepts/query/#stream-processing","text":"Stream Processing in our context is that we can choose to do a series of operations to the HStream streams. In HStreamDB, steam processing supports [TODO].","title":"Stream Processing"},{"location":"overview/concepts/stream/","text":"Stream and Streaming Data \u00b6 What is a Stream in HStreamDB? \u00b6 A stream is a flow of endless data assigned with a unique name. Streaming Data \u00b6 These endless data are known as Streaming data . Streaming data are data that are generated continuously and typically sent in data records with small sizes (order of Kilobytes). Stream \u00b6 Therefore, in a stream from HStreamDB, the default form of a single data is JSON. A flow indicates that the stream in HStreamDB is append-only, which is true. It makes no sense to delete something from a stream. Implicitly, every append operation comes with an LSN , meaning that every data in a stream is checkable and recoverable. LSN is the abbreviate for log sequence number , which is the combination of the epoch and the epoch sequence number. It is guaranteed to be monotonically increasing. Pub-Sub with Stream \u00b6 A stream can also be seen as a topic: data flows into a stream is like producers publish messages to topics. Multiple queries process data from a stream and commit checkpoint is like consumers subscribe to topics and process incoming data and send an acknowledgement. Extra \u00b6 Examples of Streaming Data \u00b6 reference: streaming-data Sensors in transportation vehicles, industrial equipment, and farm machinery send data to a streaming application. The application monitors performance, detects any potential defects in advance, and places a spare part order automatically preventing equipment down time. A financial institution tracks changes in the stock market in real time, computes value-at-risk, and automatically rebalances portfolios based on stock price movements. A real-estate website tracks a subset of data from consumers\u2019 mobile devices and makes real-time property recommendations of properties to visit based on their geo-location. A solar power company has to maintain power throughput for its customers, or pay penalties. It implemented a streaming data application that monitors of all of panels in the field, and schedules service in real time, thereby minimizing the periods of low throughput from each panel and the associated penalty payouts. A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and better experience to its audience. An online gaming company collects streaming data about player-game interactions, and feeds the data into its gaming platform. It then analyzes the data in real-time, offers incentives and dynamic experiences to engage its players.","title":"Stream and Streaming Data"},{"location":"overview/concepts/stream/#stream-and-streaming-data","text":"","title":"Stream and Streaming Data"},{"location":"overview/concepts/stream/#what-is-a-stream-in-hstreamdb","text":"A stream is a flow of endless data assigned with a unique name.","title":"What is a Stream in HStreamDB?"},{"location":"overview/concepts/stream/#streaming-data","text":"These endless data are known as Streaming data . Streaming data are data that are generated continuously and typically sent in data records with small sizes (order of Kilobytes).","title":"Streaming Data"},{"location":"overview/concepts/stream/#stream","text":"Therefore, in a stream from HStreamDB, the default form of a single data is JSON. A flow indicates that the stream in HStreamDB is append-only, which is true. It makes no sense to delete something from a stream. Implicitly, every append operation comes with an LSN , meaning that every data in a stream is checkable and recoverable. LSN is the abbreviate for log sequence number , which is the combination of the epoch and the epoch sequence number. It is guaranteed to be monotonically increasing.","title":"Stream"},{"location":"overview/concepts/stream/#pub-sub-with-stream","text":"A stream can also be seen as a topic: data flows into a stream is like producers publish messages to topics. Multiple queries process data from a stream and commit checkpoint is like consumers subscribe to topics and process incoming data and send an acknowledgement.","title":"Pub-Sub with Stream"},{"location":"overview/concepts/stream/#extra","text":"","title":"Extra"},{"location":"overview/concepts/stream/#examples-of-streaming-data","text":"reference: streaming-data Sensors in transportation vehicles, industrial equipment, and farm machinery send data to a streaming application. The application monitors performance, detects any potential defects in advance, and places a spare part order automatically preventing equipment down time. A financial institution tracks changes in the stock market in real time, computes value-at-risk, and automatically rebalances portfolios based on stock price movements. A real-estate website tracks a subset of data from consumers\u2019 mobile devices and makes real-time property recommendations of properties to visit based on their geo-location. A solar power company has to maintain power throughput for its customers, or pay penalties. It implemented a streaming data application that monitors of all of panels in the field, and schedules service in real time, thereby minimizing the periods of low throughput from each panel and the associated penalty payouts. A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and better experience to its audience. An online gaming company collects streaming data about player-game interactions, and feeds the data into its gaming platform. It then analyzes the data in real-time, offers incentives and dynamic experiences to engage its players.","title":"Examples of Streaming Data"},{"location":"reference/frequently-asked-questions/","text":"TODO \u00b6","title":"Frequently Asked Questions"},{"location":"reference/frequently-asked-questions/#todo","text":"","title":"TODO"},{"location":"reference/performance/benchmark/","text":"TODO \u00b6","title":"Performance"},{"location":"reference/performance/benchmark/#todo","text":"","title":"TODO"},{"location":"reference/security/authentication/","text":"TODO \u00b6","title":"Authentication"},{"location":"reference/security/authentication/#todo","text":"","title":"TODO"},{"location":"reference/security/checklist/","text":"TODO \u00b6","title":"Security Checklist"},{"location":"reference/security/checklist/#todo","text":"","title":"TODO"},{"location":"reference/security/encryption/","text":"TODO \u00b6","title":"Encryption"},{"location":"reference/security/encryption/#todo","text":"","title":"TODO"},{"location":"reference/security/tls-ssl/","text":"TODO \u00b6","title":"TLS/SSL"},{"location":"reference/security/tls-ssl/#todo","text":"","title":"TODO"},{"location":"reference/sql/appendix/","text":"Appendix \u00b6 Data Types \u00b6 type examples Integer 1, -1, 1234567 Double 2.3, -3.56, 232.4 Bool TRUE, FALSE Date 2020-06-10 Time 11:18:30 String \"HStreamDB \" Keywords \u00b6 keyword description AND logical and operator AS stream or field name alias AVG average function BETWEEN range operator, used with AND BY do something by certain conditions, used with GROUP or ORDER COUNT count function CREATE create a stream / connector DATE prefix of date constant DAY interval unit DROP drop a stream FORMAT specify the format of a stream FROM specify where to select data from GROUP group values by certain conditions, used with BY HAVING filter select values by a condition HOPPING hopping window INNER joining type, used with JOIN INSERT insert data into a stream, used with INTO INTERVAL prefix of interval constant INTO insert data into a stream, used with INSERT JOIN for joining two streams LEFT joining type, used with JOIN MAX maximum function MIN minimum function MINUTE interval unit MONTH interval unit NOT logical not operator ON specify conditions, used with JOIN OR logical or operator ORDER sort values by certain conditions, used with BY OUTER joining type, used with JOIN SECOND interval unit SELECT query a stream SESSION session window SHOW show something to stdout STREAM specify a stream, used with CREATE SUM sum function TIME prefix of the time constant TUMBLING tumbling window VALUES specify inserted data, used with INSERT INTO WEEK interval unit WHERE filter selected values by a condition WITH specify properties when creating a stream WITHIN specify time window when joining two streams YEAR interval unit Operators \u00b6 operator description = equal to <> not equal to < less than > greater than <= less than or equal to >= greater than or equal to + addition - subtraction * multiplication . access field of a stream [] access item of a map or an array AND logical and operator OR logical or operator NOT logical not operator BETWEEN range operator Functions \u00b6 function description AVG average COUNT count MAX maximum MIN minimum SUM sum","title":"Appendix"},{"location":"reference/sql/appendix/#appendix","text":"","title":"Appendix"},{"location":"reference/sql/appendix/#data-types","text":"type examples Integer 1, -1, 1234567 Double 2.3, -3.56, 232.4 Bool TRUE, FALSE Date 2020-06-10 Time 11:18:30 String \"HStreamDB \"","title":"Data Types"},{"location":"reference/sql/appendix/#keywords","text":"keyword description AND logical and operator AS stream or field name alias AVG average function BETWEEN range operator, used with AND BY do something by certain conditions, used with GROUP or ORDER COUNT count function CREATE create a stream / connector DATE prefix of date constant DAY interval unit DROP drop a stream FORMAT specify the format of a stream FROM specify where to select data from GROUP group values by certain conditions, used with BY HAVING filter select values by a condition HOPPING hopping window INNER joining type, used with JOIN INSERT insert data into a stream, used with INTO INTERVAL prefix of interval constant INTO insert data into a stream, used with INSERT JOIN for joining two streams LEFT joining type, used with JOIN MAX maximum function MIN minimum function MINUTE interval unit MONTH interval unit NOT logical not operator ON specify conditions, used with JOIN OR logical or operator ORDER sort values by certain conditions, used with BY OUTER joining type, used with JOIN SECOND interval unit SELECT query a stream SESSION session window SHOW show something to stdout STREAM specify a stream, used with CREATE SUM sum function TIME prefix of the time constant TUMBLING tumbling window VALUES specify inserted data, used with INSERT INTO WEEK interval unit WHERE filter selected values by a condition WITH specify properties when creating a stream WITHIN specify time window when joining two streams YEAR interval unit","title":"Keywords"},{"location":"reference/sql/appendix/#operators","text":"operator description = equal to <> not equal to < less than > greater than <= less than or equal to >= greater than or equal to + addition - subtraction * multiplication . access field of a stream [] access item of a map or an array AND logical and operator OR logical or operator NOT logical not operator BETWEEN range operator","title":"Operators"},{"location":"reference/sql/appendix/#functions","text":"function description AVG average COUNT count MAX maximum MIN minimum SUM sum","title":"Functions"},{"location":"reference/sql/sql-overview/","text":"SQL Overview \u00b6 SQL is a domain-specific language used in programming and designed for managing data held in a database management system. A standard for the specification of SQL is maintained by the American National Standards Institute (ANSI). Also, there are many variants and extensions to SQL to express more specific programs. The SQL grammar of HStreamDB is based on a subset of SQL-92 with some extensions to support stream operations. Syntax \u00b6 SQL inputs are made up of a series of statements. Each statement is made up of a series of tokens and ends in a semicolon ( ; ). A token can be a keyword argument, an identifier, a literal, an operator, or a special character. The details of the rules can be found in the BNFC grammar file or generated alex file . Normally, tokens are separated by whitespace. The following examples are syntactically valid SQL statements: SELECT * FROM my_stream ; CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 WITH ( REPLICATE = 3 ); INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 ); Keywords \u00b6 Some tokens such as SELECT , INSERT and WHERE are reserved keywords , which have specific meanings in SQL syntax. Keywords are case insensitive, which means that SELECT and select are equivalent. A keyword can not be used as an identifier. For a complete list of keywords, see the appendix . Identifiers \u00b6 Identifiers are tokens that represent user-defined objects such as streams, fields, and other ones. For example, my_stream can be used as a stream name, and temperature can represent a field in the stream. By now, identifiers only support C-style naming rules. It means that an identifier name can only have letters (both uppercase and lowercase letters), digits, and the underscore. Besides, the first letter of an identifier should be either a letter or an underscore. By now, identifiers are case sensitive, which means that my_stream and MY_STREAM are different identifiers. Literals (Constants) \u00b6 Literals are objects with known values before being executed. There are six types of constants: integers, floats, strings, dates, time, and intervals so far. Integers \u00b6 Integers are in the form of digits , where digits are one or more single-digit integers (0 through 9). Negatives such as -1 are also supported. Note that scientific notation is not supported yet . Floats \u00b6 Floats are in the form of digits . digits . Negative floats such as -11.514 are supported. Note that scientific notation is not supported yet . Forms such as 1. and .99 are not supported yet . Strings \u00b6 Strings are arbitrary character series surrounded by double quotes ( \" ), such as \"JSON\" . Dates \u00b6 Dates represent a date exact to a day in the form of DATE <year>-<month>-<day> , where <year> , <month> and <day> are all integer constants. Note that the leading DATE should not be omitted. Example: DATE 2021-01-02 Time \u00b6 Time constants represent time exact to a second in the form of TIME <hour>-<minute>-<second> , where <hour> , <minute> and <second> are all integer constants. Note that the leading TIME should not be omitted. Example: TIME 11:45:14 Intervals \u00b6 Intervals represent a time section in the form of INTERVAL <num> <time_unit> , where <num> is an integer constant and <time_unit> is one of YEAR , MONTH , WEEK , DAY , MINUTE and SECOND . Note that the leading INTERVAL should not be omitted. Example: INTERVAL 5 SECOND Operators and Functions \u00b6 Functions are special keywords that mean some computation, such as SUM and MIN . And operators are infix functions composed of special characters, such as >= and <> . For a complete list of functions and operators, see the appendix . Special characters \u00b6 There are some special characters in the SQL syntax with particular meanings: Parentheses ( () ) are used outside an expression for controlling the order of evaluation or specifying a function application. Brackets ( [] ) are used with maps and arrays for accessing their substructures, such as some_map[temp] and some_array[1] . Note that it is not supported yet . Commas ( , ) are used for delineating a list of objects. The semicolons ( ; ) represent the end of a SQL statement. The asterisk ( * ) represents \"all fields\", such as SELECT * FROM my_stream; . The period ( . ) is used for accessing a field in a stream, such as my_stream.humidity . Comments \u00b6 A single-line comment begins with // : // This is a comment Also, C-style multi-line comments are supported: /* This is another comment */","title":"SQL overview"},{"location":"reference/sql/sql-overview/#sql-overview","text":"SQL is a domain-specific language used in programming and designed for managing data held in a database management system. A standard for the specification of SQL is maintained by the American National Standards Institute (ANSI). Also, there are many variants and extensions to SQL to express more specific programs. The SQL grammar of HStreamDB is based on a subset of SQL-92 with some extensions to support stream operations.","title":"SQL Overview"},{"location":"reference/sql/sql-overview/#syntax","text":"SQL inputs are made up of a series of statements. Each statement is made up of a series of tokens and ends in a semicolon ( ; ). A token can be a keyword argument, an identifier, a literal, an operator, or a special character. The details of the rules can be found in the BNFC grammar file or generated alex file . Normally, tokens are separated by whitespace. The following examples are syntactically valid SQL statements: SELECT * FROM my_stream ; CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 WITH ( REPLICATE = 3 ); INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 );","title":"Syntax"},{"location":"reference/sql/sql-overview/#keywords","text":"Some tokens such as SELECT , INSERT and WHERE are reserved keywords , which have specific meanings in SQL syntax. Keywords are case insensitive, which means that SELECT and select are equivalent. A keyword can not be used as an identifier. For a complete list of keywords, see the appendix .","title":"Keywords"},{"location":"reference/sql/sql-overview/#identifiers","text":"Identifiers are tokens that represent user-defined objects such as streams, fields, and other ones. For example, my_stream can be used as a stream name, and temperature can represent a field in the stream. By now, identifiers only support C-style naming rules. It means that an identifier name can only have letters (both uppercase and lowercase letters), digits, and the underscore. Besides, the first letter of an identifier should be either a letter or an underscore. By now, identifiers are case sensitive, which means that my_stream and MY_STREAM are different identifiers.","title":"Identifiers"},{"location":"reference/sql/sql-overview/#literals-constants","text":"Literals are objects with known values before being executed. There are six types of constants: integers, floats, strings, dates, time, and intervals so far.","title":"Literals (Constants)"},{"location":"reference/sql/sql-overview/#integers","text":"Integers are in the form of digits , where digits are one or more single-digit integers (0 through 9). Negatives such as -1 are also supported. Note that scientific notation is not supported yet .","title":"Integers"},{"location":"reference/sql/sql-overview/#floats","text":"Floats are in the form of digits . digits . Negative floats such as -11.514 are supported. Note that scientific notation is not supported yet . Forms such as 1. and .99 are not supported yet .","title":"Floats"},{"location":"reference/sql/sql-overview/#strings","text":"Strings are arbitrary character series surrounded by double quotes ( \" ), such as \"JSON\" .","title":"Strings"},{"location":"reference/sql/sql-overview/#dates","text":"Dates represent a date exact to a day in the form of DATE <year>-<month>-<day> , where <year> , <month> and <day> are all integer constants. Note that the leading DATE should not be omitted. Example: DATE 2021-01-02","title":"Dates"},{"location":"reference/sql/sql-overview/#time","text":"Time constants represent time exact to a second in the form of TIME <hour>-<minute>-<second> , where <hour> , <minute> and <second> are all integer constants. Note that the leading TIME should not be omitted. Example: TIME 11:45:14","title":"Time"},{"location":"reference/sql/sql-overview/#intervals","text":"Intervals represent a time section in the form of INTERVAL <num> <time_unit> , where <num> is an integer constant and <time_unit> is one of YEAR , MONTH , WEEK , DAY , MINUTE and SECOND . Note that the leading INTERVAL should not be omitted. Example: INTERVAL 5 SECOND","title":"Intervals"},{"location":"reference/sql/sql-overview/#operators-and-functions","text":"Functions are special keywords that mean some computation, such as SUM and MIN . And operators are infix functions composed of special characters, such as >= and <> . For a complete list of functions and operators, see the appendix .","title":"Operators and Functions"},{"location":"reference/sql/sql-overview/#special-characters","text":"There are some special characters in the SQL syntax with particular meanings: Parentheses ( () ) are used outside an expression for controlling the order of evaluation or specifying a function application. Brackets ( [] ) are used with maps and arrays for accessing their substructures, such as some_map[temp] and some_array[1] . Note that it is not supported yet . Commas ( , ) are used for delineating a list of objects. The semicolons ( ; ) represent the end of a SQL statement. The asterisk ( * ) represents \"all fields\", such as SELECT * FROM my_stream; . The period ( . ) is used for accessing a field in a stream, such as my_stream.humidity .","title":"Special characters"},{"location":"reference/sql/sql-overview/#comments","text":"A single-line comment begins with // : // This is a comment Also, C-style multi-line comments are supported: /* This is another comment */","title":"Comments"},{"location":"reference/sql/sql-quick-reference/","text":"SQL quick reference \u00b6 CREATE STREAM \u00b6 Create a new HStreamDB stream with stream name given. An exception will be thrown if the stream is already created. See CREATE STREAM . CREATE STREAM stream_name [ AS select_query ] [ WITH ( stream_option [, ...])]; CREATE VIEW \u00b6 Create a new view with view name given. A view is a physical object like a stream and it is updated with time. An exception will be thrown if the view is already created. The name of a view can either be the same as a stream. See CREATE VIEW CREATE VIEW view_name AS select_query ; CREATE CONNECTOR \u00b6 Create a new connector for fetching data from or writing data to an external system. A connector can be either a source or a sink one. Note that source connector is not supported yet . When creating a connector, its type and binded stream must be specified in the WITH clause. There can be other options such as database name, user name and password. There can be an optional IF NOT EXIST config to only create the given connector if it does not exist. See CREATE CONNECTOR CREATE CONNECTOR < SOURCE | SINK > CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_option [, ...]); SELECT (from streams) \u00b6 Continuously get records from the stream(s) specified as streaming data flows in. It is usually used in an interactive CLI to monitor realtime changes of data. Note that the query writes these records to a random-named stream. See SELECT (Stream) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ; SELECT (from views) \u00b6 Get a record from the specified view. The fields to get have to be already in the view. It produces one or zero static record and costs little time. See Select (View) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM view_name WHERE field_name = value_expression ; INSERT \u00b6 Insert data into the specified stream. It can be a data record, a JSON value or binary data. See INSERT . INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); INSERT INTO stream_name VALUES 'json_value' ; INSERT INTO stream_name VALUES \"binary_value\" ; DROP \u00b6 Delete a given stream or view. There can be an optional IF EXISTS config to only delete the given category if it exists. DROP STREAM stream_name [ IF EXISTS ]; DROP VIEW view_name [ IF EXISTS ]; SHOW \u00b6 Show the information of all streams, queries, views or connectors. SHOW STREAMS ; SHOW QUERIES ; SHOW VIEWS ; SHOW CONNECTORS ;","title":"SQL quick reference"},{"location":"reference/sql/sql-quick-reference/#sql-quick-reference","text":"","title":"SQL quick reference"},{"location":"reference/sql/sql-quick-reference/#create-stream","text":"Create a new HStreamDB stream with stream name given. An exception will be thrown if the stream is already created. See CREATE STREAM . CREATE STREAM stream_name [ AS select_query ] [ WITH ( stream_option [, ...])];","title":"CREATE STREAM"},{"location":"reference/sql/sql-quick-reference/#create-view","text":"Create a new view with view name given. A view is a physical object like a stream and it is updated with time. An exception will be thrown if the view is already created. The name of a view can either be the same as a stream. See CREATE VIEW CREATE VIEW view_name AS select_query ;","title":"CREATE VIEW"},{"location":"reference/sql/sql-quick-reference/#create-connector","text":"Create a new connector for fetching data from or writing data to an external system. A connector can be either a source or a sink one. Note that source connector is not supported yet . When creating a connector, its type and binded stream must be specified in the WITH clause. There can be other options such as database name, user name and password. There can be an optional IF NOT EXIST config to only create the given connector if it does not exist. See CREATE CONNECTOR CREATE CONNECTOR < SOURCE | SINK > CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_option [, ...]);","title":"CREATE CONNECTOR"},{"location":"reference/sql/sql-quick-reference/#select-from-streams","text":"Continuously get records from the stream(s) specified as streaming data flows in. It is usually used in an interactive CLI to monitor realtime changes of data. Note that the query writes these records to a random-named stream. See SELECT (Stream) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ;","title":"SELECT (from streams)"},{"location":"reference/sql/sql-quick-reference/#select-from-views","text":"Get a record from the specified view. The fields to get have to be already in the view. It produces one or zero static record and costs little time. See Select (View) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM view_name WHERE field_name = value_expression ;","title":"SELECT (from views)"},{"location":"reference/sql/sql-quick-reference/#insert","text":"Insert data into the specified stream. It can be a data record, a JSON value or binary data. See INSERT . INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); INSERT INTO stream_name VALUES 'json_value' ; INSERT INTO stream_name VALUES \"binary_value\" ;","title":"INSERT"},{"location":"reference/sql/sql-quick-reference/#drop","text":"Delete a given stream or view. There can be an optional IF EXISTS config to only delete the given category if it exists. DROP STREAM stream_name [ IF EXISTS ]; DROP VIEW view_name [ IF EXISTS ];","title":"DROP"},{"location":"reference/sql/sql-quick-reference/#show","text":"Show the information of all streams, queries, views or connectors. SHOW STREAMS ; SHOW QUERIES ; SHOW VIEWS ; SHOW CONNECTORS ;","title":"SHOW"},{"location":"reference/sql/statements/create-stream/","text":"CREATE STREAM \u00b6 Register a stream on the bottom layer topic with the same name as the stream. An exception will be thrown if the stream is already created. Synopsis \u00b6 CREATE STREAM stream_name [ AS select_query ] WITH ( REPLICATE = INT ); Notes \u00b6 stream_name is a valid identifier. select_query is an optional SELECT (Stream) query. For more information, see SELECT section. When <select_query> is specified, the created stream will be filled with records from the SELECT query continuously. Otherwise, the stream will only be created and kept empty. Examples \u00b6 CREATE STREAM weather WITH ( FORMAT = \"JSON\" ); CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 EMIT CHANGES WITH ( FORMAT = \"JSON\" );","title":"CREATE STREAM"},{"location":"reference/sql/statements/create-stream/#create-stream","text":"Register a stream on the bottom layer topic with the same name as the stream. An exception will be thrown if the stream is already created.","title":"CREATE STREAM"},{"location":"reference/sql/statements/create-stream/#synopsis","text":"CREATE STREAM stream_name [ AS select_query ] WITH ( REPLICATE = INT );","title":"Synopsis"},{"location":"reference/sql/statements/create-stream/#notes","text":"stream_name is a valid identifier. select_query is an optional SELECT (Stream) query. For more information, see SELECT section. When <select_query> is specified, the created stream will be filled with records from the SELECT query continuously. Otherwise, the stream will only be created and kept empty.","title":"Notes"},{"location":"reference/sql/statements/create-stream/#examples","text":"CREATE STREAM weather WITH ( FORMAT = \"JSON\" ); CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 EMIT CHANGES WITH ( FORMAT = \"JSON\" );","title":"Examples"},{"location":"reference/sql/statements/insert/","text":"INSERT \u00b6 Insert a record into specified stream. Synopsis \u00b6 INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); Notes \u00b6 field_value represents the value of corresponding field, which is a constant . The correspondence between field type and inserted value is maintained by users themselves. Examples \u00b6 INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 );","title":"INSERT"},{"location":"reference/sql/statements/insert/#insert","text":"Insert a record into specified stream.","title":"INSERT"},{"location":"reference/sql/statements/insert/#synopsis","text":"INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]);","title":"Synopsis"},{"location":"reference/sql/statements/insert/#notes","text":"field_value represents the value of corresponding field, which is a constant . The correspondence between field type and inserted value is maintained by users themselves.","title":"Notes"},{"location":"reference/sql/statements/insert/#examples","text":"INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 );","title":"Examples"},{"location":"reference/sql/statements/select-stream/","text":"SELECT (Stream) \u00b6 Continuously pulls records from the stream(s) specified. It is usually used in an interactive CLI to monitor realtime changes of data. Note that the query writes records to a random-named stream. Synopsis \u00b6 SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ; Notes \u00b6 expression can be a field name, a constant, or their association, such as temperature , weather.humidity , 114514 , 1 + 2 and SUM(productions) . some_interval represents a period of time. See Intervals . join_type specifies the type of joining operation. Only INNER is supported yet. window_type specifies the type of time window: window_type ::= TUMBLING some_interval | HOPPING some_interval some_interval | SESSION some_interval search_condition is actually a boolean expression: search_condition ::= [NOT] predicate [ <AND | OR> predicate [, ...] ] predicate ::= expression comp_op expression comp_op ::= = | <> | > | < | >= | <= Examples \u00b6 A simple query: SELECT * FROM my_stream EMIT CHANGES ; Filtering rows: SELECT temperature , humidity FROM weather WHERE temperature > 10 AND humidity < 75 EMIT CHANGES ; Joining streams: SELECT stream1 . temperature , stream2 . humidity FROM stream1 INNER JOIN stream2 WITHIN ( INTERVAL 5 SECOND ) ON stream1 . humidity = stream2 . humidity EMIT CHANGES ; Grouping records: SELECT COUNT ( * ) FROM weather GROUP BY cityId , TUMBLING ( INTERVAL 10 SECOND ) EMIT CHANGES ;","title":"SELECT (Stream)"},{"location":"reference/sql/statements/select-stream/#select-stream","text":"Continuously pulls records from the stream(s) specified. It is usually used in an interactive CLI to monitor realtime changes of data. Note that the query writes records to a random-named stream.","title":"SELECT (Stream)"},{"location":"reference/sql/statements/select-stream/#synopsis","text":"SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ;","title":"Synopsis"},{"location":"reference/sql/statements/select-stream/#notes","text":"expression can be a field name, a constant, or their association, such as temperature , weather.humidity , 114514 , 1 + 2 and SUM(productions) . some_interval represents a period of time. See Intervals . join_type specifies the type of joining operation. Only INNER is supported yet. window_type specifies the type of time window: window_type ::= TUMBLING some_interval | HOPPING some_interval some_interval | SESSION some_interval search_condition is actually a boolean expression: search_condition ::= [NOT] predicate [ <AND | OR> predicate [, ...] ] predicate ::= expression comp_op expression comp_op ::= = | <> | > | < | >= | <=","title":"Notes"},{"location":"reference/sql/statements/select-stream/#examples","text":"A simple query: SELECT * FROM my_stream EMIT CHANGES ; Filtering rows: SELECT temperature , humidity FROM weather WHERE temperature > 10 AND humidity < 75 EMIT CHANGES ; Joining streams: SELECT stream1 . temperature , stream2 . humidity FROM stream1 INNER JOIN stream2 WITHIN ( INTERVAL 5 SECOND ) ON stream1 . humidity = stream2 . humidity EMIT CHANGES ; Grouping records: SELECT COUNT ( * ) FROM weather GROUP BY cityId , TUMBLING ( INTERVAL 10 SECOND ) EMIT CHANGES ;","title":"Examples"},{"location":"start/basic-commands/","text":"Basic Commands \u00b6 TODO","title":"Basic Commands"},{"location":"start/basic-commands/#basic-commands","text":"TODO","title":"Basic Commands"},{"location":"start/configuration-overview/","text":"TODO \u00b6","title":"Configurarion Overview"},{"location":"start/configuration-overview/#todo","text":"","title":"TODO"},{"location":"start/dashboard-overview/","text":"TODO \u00b6","title":"Dashboard Overview"},{"location":"start/dashboard-overview/#todo","text":"","title":"TODO"},{"location":"start/quickstart-with-docker/","text":"Quickstart with Docker \u00b6 Installation \u00b6 Install docker \u00b6 Note If you have already installed docker, you can skip this step. See Install Docker Engine , and install it for your operating system. Please carefully check that you meet all prerequisites. Confirm that the Docker daemon is running: docker version Tips On Linux, Docker needs root privileges. You can also run Docker as a non-root user, see Post-installation steps for Linux . Pull docker images \u00b6 docker pull hstreamdb/hstream Start a local standalone HStream-Server in Docker \u00b6 !!! warning Do NOT use this configuration in your production environment! Create a directory for storing db datas \u00b6 mkdir /dbdata Tips If you are a non-root user, that you can not create directory under the root, you can also create it anywhere as you can, but you need to pass the absolute data path to docker volume arguments. Start HStream Storage \u00b6 docker run -td --rm --name some-hstream-store -v /dbdata:/data/store --network host hstreamdb/hstream ld-dev-cluster --root /data/store --use-tcp Start HStreamDB Server \u00b6 docker run -it --rm --name some-hstream-server -v /dbdata:/data/store --network host hstreamdb/hstream hstream-server --port 6570 --store-config /data/store/logdevice.conf Start HStreamDB's interactive SQL CLI \u00b6 docker run -it --rm --name some-hstream-cli -v /dbdata:/data/store --network host hstreamdb/hstream hstream-client --port 6570 If everything works fine, you will enter an interactive CLI and see help information like / / / / ___/_ __/ __ \\/ ____/ | / |/ / / /_/ /\\__ \\ / / / /_/ / __/ / /| | / /|_/ / / __ /___/ // / / _, _/ /___/ ___ |/ / / / /_/ /_//____//_/ /_/ |_/_____/_/ |_/_/ /_/ Command :h help command :q quit cli <sql> run sql > Create a stream \u00b6 What we are going to do first is create a stream by CREATE STREAM query. CREATE STREAM demo ; Run a continuous query over the stream \u00b6 Now we can run a continuous query over the stream we just created by SELECT query. The query will output all records from the demo stream whose humidity is above 70 percent. SELECT * FROM demo WHERE humidity > 70 EMIT CHANGES ; It seems that nothing happened. But do not worry because there is no data in the stream now. Next, we will fill the stream with some data so the query can produce output we want. Start another CLI session \u00b6 Start another CLI session, this CLI will be used for inserting data into the stream. docker exec -it some-hstream-cli hstream-client --port 6570 Insert data into the stream \u00b6 Run each of the given INSERT query in the new CLI session and keep an eye on the CLI session created in (2). INSERT INTO demo ( temperature , humidity ) VALUES ( 22 , 80 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 15 , 20 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 31 , 76 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 5 , 45 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 27 , 82 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 28 , 86 ); If everything works fine, the continuous query will output matching records in real time: {\"temperature\":22,\"humidity\":80} {\"temperature\":31,\"humidity\":76} {\"temperature\":27,\"humidity\":82} {\"temperature\":28,\"humidity\":86}","title":"Quickstart with Docker"},{"location":"start/quickstart-with-docker/#quickstart-with-docker","text":"","title":"Quickstart with Docker"},{"location":"start/quickstart-with-docker/#installation","text":"","title":"Installation"},{"location":"start/quickstart-with-docker/#install-docker","text":"Note If you have already installed docker, you can skip this step. See Install Docker Engine , and install it for your operating system. Please carefully check that you meet all prerequisites. Confirm that the Docker daemon is running: docker version Tips On Linux, Docker needs root privileges. You can also run Docker as a non-root user, see Post-installation steps for Linux .","title":"Install docker"},{"location":"start/quickstart-with-docker/#pull-docker-images","text":"docker pull hstreamdb/hstream","title":"Pull docker images"},{"location":"start/quickstart-with-docker/#start-a-local-standalone-hstream-server-in-docker","text":"!!! warning Do NOT use this configuration in your production environment!","title":"Start a local standalone HStream-Server in Docker"},{"location":"start/quickstart-with-docker/#create-a-directory-for-storing-db-datas","text":"mkdir /dbdata Tips If you are a non-root user, that you can not create directory under the root, you can also create it anywhere as you can, but you need to pass the absolute data path to docker volume arguments.","title":"Create a directory for storing db datas"},{"location":"start/quickstart-with-docker/#start-hstream-storage","text":"docker run -td --rm --name some-hstream-store -v /dbdata:/data/store --network host hstreamdb/hstream ld-dev-cluster --root /data/store --use-tcp","title":"Start HStream Storage"},{"location":"start/quickstart-with-docker/#start-hstreamdb-server","text":"docker run -it --rm --name some-hstream-server -v /dbdata:/data/store --network host hstreamdb/hstream hstream-server --port 6570 --store-config /data/store/logdevice.conf","title":"Start HStreamDB Server"},{"location":"start/quickstart-with-docker/#start-hstreamdbs-interactive-sql-cli","text":"docker run -it --rm --name some-hstream-cli -v /dbdata:/data/store --network host hstreamdb/hstream hstream-client --port 6570 If everything works fine, you will enter an interactive CLI and see help information like / / / / ___/_ __/ __ \\/ ____/ | / |/ / / /_/ /\\__ \\ / / / /_/ / __/ / /| | / /|_/ / / __ /___/ // / / _, _/ /___/ ___ |/ / / / /_/ /_//____//_/ /_/ |_/_____/_/ |_/_/ /_/ Command :h help command :q quit cli <sql> run sql >","title":"Start HStreamDB's interactive SQL CLI"},{"location":"start/quickstart-with-docker/#create-a-stream","text":"What we are going to do first is create a stream by CREATE STREAM query. CREATE STREAM demo ;","title":"Create a stream"},{"location":"start/quickstart-with-docker/#run-a-continuous-query-over-the-stream","text":"Now we can run a continuous query over the stream we just created by SELECT query. The query will output all records from the demo stream whose humidity is above 70 percent. SELECT * FROM demo WHERE humidity > 70 EMIT CHANGES ; It seems that nothing happened. But do not worry because there is no data in the stream now. Next, we will fill the stream with some data so the query can produce output we want.","title":"Run a continuous query over the stream"},{"location":"start/quickstart-with-docker/#start-another-cli-session","text":"Start another CLI session, this CLI will be used for inserting data into the stream. docker exec -it some-hstream-cli hstream-client --port 6570","title":"Start another CLI session"},{"location":"start/quickstart-with-docker/#insert-data-into-the-stream","text":"Run each of the given INSERT query in the new CLI session and keep an eye on the CLI session created in (2). INSERT INTO demo ( temperature , humidity ) VALUES ( 22 , 80 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 15 , 20 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 31 , 76 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 5 , 45 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 27 , 82 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 28 , 86 ); If everything works fine, the continuous query will output matching records in real time: {\"temperature\":22,\"humidity\":80} {\"temperature\":31,\"humidity\":76} {\"temperature\":27,\"humidity\":82} {\"temperature\":28,\"humidity\":86}","title":"Insert data into the stream"}]}